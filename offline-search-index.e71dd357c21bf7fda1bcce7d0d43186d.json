[{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/","tags":"","title":"CloudNative"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/monitoring/prometheus/introduction/","tags":"","title":"Introduction"},{"body":"什么是 Prometheus？ Prometheus 是最初在 SoundCloud 上构建的开源系统监视和警报工具包。自 2012 年成立以来，许多公司和组织都采用了 Prometheus，该项目拥有非常活跃的开发人员和用户社区。现在，它是一个独立的开源项目，并且独立于任何公司进行维护。为了强调这一点并阐明项目的治理结构，Prometheus 在 2016 年加入了 Cloud Native Computing Foundation ，这是继 Kubernetes 之后的第二个托管项目。有关 Prometheus 的详细说明，请参见 media 部分中的资源链接。\n特性 Prometheus 的主要特性是：\n一个多维数据模型，其中包含通过指标名称和键/值对标识的时间序列数据。 PromQL，一种灵活的查询语言，可利用此维度。 不依赖分布式存储；单服务器节点是自治的。 时间序列收集通过 HTTP 上的拉模型进行。 通过中间网关支持推送时间序列。 通过服务发现或静态配置发现目标。 支持多种模式的图形和仪表板 组件 Prometheus 生态系统由多个组件组成，其中许多是可选的：\n主要的 Prometheus server 用于搜集并存储时间序列数据。 client libraries 用于检测应用程序代码。 push gateway 用于支持短期工作 服务专用的 exporters，比如用于 HAProxy、StatsD、Graphite等服务。 alertmanager 用于处理警报。 各种支持工具 大多数 Prometheus 组件都是用 Go 编写的，因此易于构建和部署为静态二进制文件。\n架构 下图说明了 Prometheus 的体系结构及其某些生态系统组件：\nPrometheus 从已检测作业中搜集指标，或是直接地，或是通过中间推送网关处理短期工作。它将所有搜集的样品存储在本地，并对这些数据运行规则，以从现有数据中汇总和记录新时间序列，或生成警报。Grafana 或其他 API 使用者可以用来可视化收集的数据。\n什么时候适合？ Prometheus 可以很好地记录任何纯数字时间序列。它既适用于以机器为中心的监控，也适用于高度动态的面向服务的体系结构的监控。在微服务世界中，它对多维数据收集和查询的支持是一种特别的优势。\nPrometheus 专为可靠性而设计，成为您要使用的系统，该系统帮助您在中断期间能够快速诊断问题。每个 Prometheus server 都是独立的，而不依赖于网络存储或其他远程服务。当基础结构的其他部分损坏时，您可以依靠它，并且无需设置广泛的基础结构即可使用它。\n什么时候不适合？ Prometheus 重视可靠性。即使在故障情况下，您始终可以查看有关系统的可用统计信息。如果您需要 100％ 的准确性（例如按请求计费），则 Prometheus 并不是一个很好的选择，因为所收集的数据可能不够详细和完整。在这种情况下，最好使用其他系统来收集和分析数据以进行计费，并使用 Prometheus 进行其余的监视。\n","categories":"","description":"","excerpt":"什么是 Prometheus？ Prometheus 是最初在 SoundCloud 上构建的开源系统监视和警报工具包。自 2012 年成立 …","ref":"/docs/monitoring/prometheus/introduction/overview/","tags":"","title":"Overview"},{"body":"kubernetes-docs 是作者学习 Kubernetes v1.18 官方文档 所翻译的中文文档，大部分内容与 Kubernetes v1.18 官方中文文档 一致，在此基础上做了少部分修正。\n目录导航 概念篇 概述 Kubernetes 是什么 Kubernetes 组件 集群架构 控制器 工作负载 Pods Pod 概述 Pods Pod 生命周期 Init 容器 Pod Preset Pod 拓扑扩展约束 干扰 参考链接 Kubernetes 官方文档 Kubernetes 官方中文文档 ","categories":"","description":"","excerpt":"kubernetes-docs 是作者学习 Kubernetes v1.18 官方文档 所翻译的中文文档，大部分内容与 Kubernetes …","ref":"/docs/cloudnative/kubernetes/","tags":"","title":"Kubernetes 文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/envoy/introduction/","tags":"","title":"Introduction"},{"body":"Envoy 是 L7 代理和通信总线，专为面向大型现代服务的体系结构而设计。该项目是基于以下信念而诞生的：\n网络对应用程序应该是透明的。当确实发生网络和应用程序问题时，应该容易确定问题的根源。\n实际上，实现上述目标非常困难。 Envoy 尝试通过提供以下高级功能来做到这一点：\n**进程外架构：**Envoy 是一个自包含的进程，被设计为与每个应用程序服务器一起运行。所有的 Envoy 组成一个透明的通信网格，每个应用程序在其中都与 localhost 之间发送和接收消息，并且不知道网络拓扑。与传统的库实现服务到服务通信方法相比，进程外架构具有两个实质性的好处：\nEnvoy 可与任何应用程序语言一起工作。单个 Envoy 部署可以在 Java、C ++、Go、PHP、Python 等之间形成网格。面向服务的架构（SOA）使用多种应用程序框架和语言变得越来越普遍。Envoy 透明地弥合了其中的差距。 任何使用大型面向服务的架构的人都知道，部署库升级可能会非常痛苦。Envoy 可以透明地在整个基础架构中快速部署和升级。 Modern C++11 code base: Envoy 用 C++11 编写。选择原生代码是因为我们认为应尽可能避免使用 Envoy 之类的架构组件。现代应用开发人员已经处理了因共享云环境中的部署而难以推理的尾部延迟，以及使用效率很高但性能不是很好的语言，例如 PHP、Python、Ruby、Scala 等。原生代码通常提供出色的延迟属性，不会给已经令人困惑的情况带来额外的混乱。与其他用 C 编写的原生代码代理解决方案不同，C++11 提供了出色的开发人员生产力和性能。\n**L3/L4 过滤器架构：**Envoy 的核心是 L3/L4网络代理。可插拔的 过滤器 链机制允许写入过滤器以执行不同的 TCP 代理任务，并将其插入主服务器。已经写好了过滤器来支持各种任务，例如原始 TCP 代理、HTTP 代理、TLS 客户端证书认证等。\nHTTP L7 过滤器架构： HTTP 是现代应用架构的关键组件，Envoy 支持额外的 HTTP L7 过滤器层。HTTP 过滤器可以插入 HTTP 连接管理子系统中，该子系统执行不同的任务，例如缓冲、速率限制、路由/转发、嗅探 Amazon 的 DynamoDB 等。\n**一流的 HTTP/2 支持：**在 HTTP 模式下运行时，Envoy 支持 HTTP/1.1 和 HTTP/2。Envoy 可以在两个方向上充当透明的 HTTP/1.1 到 HTTP/2 代理。这意味着可以桥接 HTTP/1.1 和 HTTP/2 客户端与目标服务器的任何组合。推荐的服务到服务配置是在所有 Envoy 之间使用 HTTP/2 来创建持久连接的网格，该请求和响应可以多路复用。由于 SPDY 协议正在逐步淘汰，Envoy 不支持 SPDY。\n**HTTP L7 路由：**在 HTTP 模式下运行时，Envoy 支持路由子系统，该子系统能够基于路径、权限、内容类型、运行时值等来路由和重定向请求。当使用 Envoy 作为前端/边缘代理时，此功能最有用，但在构建服务到服务网格的服务时也可以利用此功能。\ngRPC 支持：gRPC 是 Google 的 RPC框架，使用 HTTP/2 作为基础的多路复用传输。Envoy 支持所有 HTTP/2 功能，这些功能必须用作 gRPC 请求和响应的路由和负载平衡基础。这两个系统是非常互补的。\nMongoDB L7 支持：MongoDB 是在现代 Web 应用程序中使用的流行数据库。Envoy 支持 L7 嗅探，统计信息生成和 MongoDB 连接的日志记录。\nDynamoDB L7支持：DynamoDB 是 Amazon 托管的键/值 NOSQL 数据存储。 Envoy 支持 DynamoDB 连接的 L7 嗅探和统计信息生成。\n**服务发现和动态配置：**Envoy 可选地使用一组分层的动态配置 API 进行集中管理。这些层为 Envoy 提供有关以下方面的动态更新：后端集群中的主机，后端集群本身，HTTP 路由，侦听套接字和加密材料。对于更简单的部署，后端主机发现可以通过 DNS 解析（甚至完全跳过）来完成，而将其他层替换为静态配置文件。\n**健康检查：**构建 Envoy 网格的推荐方法是将服务发现视为最终一致的过程。Envoy 包括健康检查子系统，该子系统可以有选择地对上游服务集群执行主动健康检查。然后，Envoy 使用服务发现和健康检查信息的结合来确定健康的负载均衡目标。Envoy 还通过异常值检测子系统支持被动健康检查。\n**高级负载均衡：**分布式系统中不同组件之间的负载均衡是一个复杂的问题。因为 Envoy 是一个自包含的代理而不是一个库，所以它能够在一个地方实现高级负载均衡技术，并使任何应用程序都可以访问它们。目前，Envoy 包括对自动重试、断路、通过外部速率限制服务进行全局速率限制、请求屏蔽和异常检测的支持。计划为请求竞赛提供未来的支持。\n**前端/边缘代理支持：**尽管 Envoy 最初主要被设计为服务到服务的通信系统，但在边缘使用相同的软件会有所帮助（可观察性，管理，相同的服务发现和负载平衡算法等）。Envoy 包含足够的功能，使其可以用作大多数现代 Web 应用程序用例的边缘代理。这包括 TLS 终止，HTTP/1.1 和 HTTP/2 支持以及 HTTP L7 路由。\n**一流的可观察性：**如上所述，Envoy 的主要目标是使网络透明。但是，在网络级别和应用程序级别都会发生问题。 Envoy 包括对所有子系统的强大的统计支持。statsd（和兼容的提供程序）是当前受支持的统计接收器，尽管插入另一个并不困难。可以通过管理端口查看统计信息。 Envoy 还支持通过第三方提供商进行分布式跟踪。\n设计目标 关于代码本身的设计目标的简短说明：尽管Envoy绝不慢（我们花了很多时间来优化某些快速路径），但是代码的编写是模块化的，易于测试，而不是追求最大的绝对性能。我们认为，这是一种更有效的时间利用方式，因为典型的部署将与语言和运行时同时出现，速度降低许多倍，而内存使用却增加许多倍。\n","categories":"","description":"","excerpt":"Envoy 是 L7 代理和通信总线，专为面向大型现代服务的体系结构而设计。该项目是基于以下信念而诞生的：\n网络对应用程序应该是透明的。当确 …","ref":"/docs/cloudnative/envoy/introduction/what-is-envoy/","tags":"","title":"What is Envoy"},{"body":"欢迎来到 Prometheus！Prometheus 是一个监控平台，它通过在这些目标上搜集指标 HTTP 端点来从被监视的目标收集指标数据。本指南将向您展示如何安装、配置和使用 Prometheus 监控我们的第一个资源。您将下载，安装并运行 Prometheus。您还将下载并安装 exporter，这些工具可在主机和服务上暴露时间序列数据。我们的第一个 exporter 将是 Prometheus 本身，它提供了关于内存使用，垃圾回收等各种主机级别的指标。\n下载 Prometheus 为您的平台下载最新版本的 Prometheus，然后解压缩它：\ntar xvfz prometheus-*.tar.gz cd prometheus-* Prometheus server 是一个称为 prometheus（或 prometheus.exe 在 Microsoft Windows上）的二进制文件。我们可以运行二进制文件并通过传递 --help 标志来查看有关其选项的帮助。\n./prometheus --help usage: prometheus [\u003cflags\u003e] The Prometheus monitoring server . . . 在启动 Prometheus 之前，让我们对其进行配置。\n配置 Prometheus Prometheus 配置为 YAML 格式。 Prometheus 下载包附带了示例配置在一个名为 prometheus.yml 的文件中，这个文件是开始的好地方。\n我们删除了示例文件中的大多数注释，以使其更加简洁（注释是带有 # 前缀的行）。\nglobal: scrape_interval: 15s evaluation_interval: 15s rule_files: # - \"first.rules\" # - \"second.rules\" scrape_configs: - job_name: prometheus static_configs: - targets: ['localhost:9090'] 示例配置文件中包含三个配置块：global、rule_files 和 scrape_configs。\nglobal 控制 Prometheus server 的全局配置。我们目前有两个选项。首先，scrape_interval 控制 Prometheus 多久搜集一次目标。您可以为单个目标覆盖此选项。在这个例子中，全局设置是每 15 秒搜集一次。evaluation_interval 选项控制 Prometheus 多久评估一次 rule。Prometheus 使用 rule 来创建新的时间序列并生成警报。\nrule_files 块指定我们要 Prometheus server 加载的任何规则的位置。目前，我们还没有任何规则。\n最后一块 scrape_configs 控制 Prometheus 监控哪些资源。由于 Prometheus 还将有关自身的数据公开为 HTTP 端点，因此它可以抓取并监视其自身的运行状况。在默认配置中，有一个名为 prometheus 的作业，它会抓取 Prometheus server 公开的时间序列数据。该作业包含了单个静态配置的目标：'localhost:9090'。Prometheus 希望指标在目标的 /metrics 路径上是可用的。因此，此默认作业是通过以下网址进行抓取：http://localhost:9090/metrics。\n返回的时间序列数据将详细说明 Prometheus 服务器的状态和性能。\n有关配置选项的完整说明，请参阅配置文档。\n启动 Prometheus 要使用我们新创建的配置文件启动 Prometheus，切换到包含 Prometheus 二进制文件的目录并运行：\n./prometheus --config.file=prometheus.yml Prometheus 将会启动。您还应该能够在 http://localhost:9090 上浏览到有关其自身的状态页。给它大约 30 秒的时间，以从其自己的 HTTP 指标端点收集有关自身的数据。\n您还可以通过导航到它自己的指标端点：http://localhost:9090/metrics 来验证 Prometheus 是否正在提供有关其自身的指标。\n使用表达式浏览器 让我们尝试查看 Prometheus 收集到的有关自身的一些数据。要使用 Prometheus 的内置表达式浏览器，请导航至 http://localhost:9090/graph 并在 “Graph” 选项卡中选择 “Console” 视图。\n正如您可以从 http://localhost:9090/metrics 收集的那样，Prometheus 导出的有关其自身的一个指标称为 promhttp_metric_handler_requests_total （Prometheus server 已处理的 /metrics 请求总数）。继续并将以下内容输入到表达式控制台中：\npromhttp_metric_handler_requests_total 这应该返回一些不同的时间序列（以及每个时间序列的最新值），所有时间序列的指标名称均为 promhttp_metric_handler_requests_total，但具有不同的标签。这些标签指定不同的请求状态。\n如果我们只对返回 HTTP code 200 的请求感兴趣，则可以使用此查询来检索该信息：\npromhttp_metric_handler_requests_total{code=\"200\"} 要统计返回的时间序列数，您可以编写：\ncount(promhttp_metric_handler_requests_total) 有关表达语言的更多信息，请参见 expression language documentation。\n使用绘图界面 要绘制表达式的图形，请导航到 http://localhost:9090/graph 并使用 “Graph” 选项卡。\n例如，输入以下表达式以绘制在自抓取的 Prometheus 中发生的每秒返回状态码 200 的 HTTP 请求速率\nrate(promhttp_metric_handler_requests_total{code=\"200\"}[1m]) 您可以尝试使用图形 range 参数和其他设置。\n监控其他目标 仅从 Prometheus 收集指标并不能很好地说明 Prometheus 的功能。为了更好地了解 Prometheus 可以做什么，我们建议您浏览有关其他 exporter 的文档。Monitoring Linux or macOS host metrics using a node exporter 指南是一个不错的起点。\n总结 在本指南中，您安装了 Prometheus，配置了 Prometheus 实例以监视资源，并了解了在 Prometheus 表达式浏览器中使用时间序列数据的一些基础知识。要继续学习 Prometheus，请查看 Overview 以获取有关接下来要探索的内容的一些想法。\n","categories":"","description":"","excerpt":"欢迎来到 Prometheus！Prometheus 是一个监控平台，它通过在这些目标上搜集指标 HTTP 端点来从被监视的目标收集指标数 …","ref":"/docs/monitoring/prometheus/introduction/first-steps/","tags":"","title":"First Steps"},{"body":"Prometheus vs. Graphite Scope Graphite 专注于成为具有查询语言和图形功能的被动时间序列数据库。 其他任何问题都可以通过外部组件解决。\nPrometheus 是一个完整的监视和趋势分析系统，包括基于时间序列数据的内置和主动抓取、存储、查询、图形化和警报。它了解世界应该是什么样子（应该存在哪些端点，什么时间序列模式意味着麻烦等），并积极尝试查找错误。\nData model Graphite 存储命名时间序列的数值样本，就像 Prometheus 一样。但是，Prometheus 的元数据模型更加丰富：Graphite 指标名称由点分隔的成分组成，这些成分隐式地对维度进行编码，Prometheus 将维度明确编码为键值对（称为标签），并附加到度量标准名称。这允许查询语言通过这些标签轻松进行过滤，分组和匹配。\n此外，尤其是当 Graphite 与 StatsD 结合使用时，它通常只存储在所有受监视实例上的聚合数据，而不是将实例保留为一个维度并能够向下钻取到有问题的实例。\n例如，在 Graphite/StatsD 中存储对 API server 发起的 POST 方法到 /tracks 端点且响应码是 500 的 HTTP 请求数通常会这样编码：\nstats.api-server.tracks.post.500 -\u003e 93 在 Prometheus 中，相同的数据将会像这样编码（假设三个 api-server 实例）：\napi_server_http_requests_total{method=\"POST\",handler=\"/tracks\",status=\"500\",instance=\"\u003csample1\u003e\"} -\u003e 34 api_server_http_requests_total{method=\"POST\",handler=\"/tracks\",status=\"500\",instance=\"\u003csample2\u003e\"} -\u003e 28 api_server_http_requests_total{method=\"POST\",handler=\"/tracks\",status=\"500\",instance=\"\u003csample3\u003e\"} -\u003e 31 Storage Graphite 将时间序列数据以 Whisper 格式存储在本地磁盘上，\nGraphite 以 Whisper 格式将时间序列数据存储在本地磁盘上，这是一种 RRD 风格的数据库，它希望样本以固定的时间间隔到达。每个时间序列都存储在一个单独的文件中，新样本在一定时间后会覆盖旧样本。\nPrometheus 同样为每个时间序列创建一个本地文件，但允许在出现抓取或规则评估时以任意间隔存储样本。由于新样本只是简单地附加，因此旧数据可以任意保留。Prometheus 也适用于许多短暂的，经常变化的时间序列集。\nSummary Prometheus 除了更易于运行和集成到您的环境之外，还提供了更丰富的数据模型和查询语言。如果您想要一个可以长期保存历史数据的群集解决方案，那么 Graphite 可能是一个更好的选择。\nPrometheus vs. InfluxDB InfluxDB 是一个开源时间序列数据库，具有用于扩展和集群化的商业选项。Prometheus 开发开始将近一年后，InfluxDB 项目才发布，因此我们当时无法将其视为替代方案。尽管如此，Prometheus 和 InfluxDB 之间仍然存在重大差异，并且两种系统都针对稍有不同的用例。\nScope 为了进行公平的比较，我们还必须将 Kapacitor 与 InfluxDB 一起考虑，因为它们结合起来可以解决与 Prometheus 和 Alertmanager 相同的问题空间。\n与 Graphite 相同的范围差异在这里适用于 InfluxDB 本身。另外，InfluxDB 提供了连续查询，这些查询等同于 Prometheus 记录规则。\nKapacitor 的范围是 Prometheus 记录规则，警报规则和 Alertmanager 的通知功能的组合。Prometheus 提供了更强大的查询语言来进行图形显示和警报。 Prometheus Alertmanager 还提供分组，重复数据删除和静音功能。\nData model / storage 与 Prometheus 一样，InfluxDB 数据模型也使用键值对作为标签，称为 tags。此外，InfluxDB 还有第二级标签，称为字段，使用范围受到更多限制。InfluxDB 支持最高达纳秒级的时间戳，以及 float64，int64，bool 和字符串数据类型。相比之下，Prometheus 支持 float64 数据类型，有限的字符串支持和毫秒级的时间戳。\nInfluxDB 使用 log-structured merge tree for storage with a write ahead log 的变体，使用时间分片。与 Prometheus 的为每个时间序列仅附加到文件的方法相比，此方法更适合事件记录。\nInfluxDB uses a variant of a log-structured merge tree for storage with a write ahead log, sharded by time. This is much more suitable to event logging than Prometheus’s append-only file per time series approach.\nLogs and Metrics and Graphs, Oh My! 描述了事件记录和指标记录之间的区别。\nArchitecture Prometheus servers 彼此独立运行，并且仅依靠其本地存储来实现其核心功能：抓取，规则处理和警报。 InfluxDB 的开源版本与此类似。\n根据设计，商业 InfluxDB 产品是一个分布式存储集群，其中存储和查询由多个节点一次处理。\n这意味着商业 InfluxDB 将更易于水平扩展，但这也意味着您必须从一开始就管理分布式存储系统的复杂性。Prometheus 将更容易运行，但是在某些时候，您将需要按照产品，服务，数据中心或类似方面的可伸缩性边界明确地划分服务器。独立的服务（可以并行冗余运行）也可以为您提供更好的可靠性和故障隔离。\nKapacitor 的开源版本没有内置分布式/冗余选项用于规则，警报或通知。Kapacitor 的开源发行版可以通过用户手动分片来扩展，类似于 Prometheus 本身。 Influx 提供了 Enterprise Kapacitor，它支持了 HA/冗余 警报系统。\n相比之下，Prometheus 和 Alertmanager 通过运行 Prometheus 的冗余副本并使用 Alertmanager 的 High Availability 模式提供了完全开源的冗余选项。\nSummary 系统之间有许多相似之处。两者都有标签（在 InfluxDB 中称为tags）以有效支持多维指标。两者都使用基本相同的数据压缩算法。两者都有广泛的集成，包括彼此之间的集成。两者都有钩子，可让您进一步扩展它们，例如使用统计工具分析数据或执行自动化操作。\nInfluxDB 更好的地方：\n如果您要进行事件记录。 商业选项为 InfluxDB 提供集群，这对于长期数据存储也更好。 最终在副本之间保持一致的数据视图。 Prometheus 更好的地方：\n如果您主要是在做指标。 更强大的查询语言，警报和通知功能。 图形和警报的可用性和正常运行时间更高。 Where Prometheus is better:\nIf you’re primarily doing metrics. More powerful query language, alerting, and notification functionality. Higher availability and uptime for graphing and alerting. InfluxDB 由一家商业公司按照开放核心模型进行维护，并提供高级功能，例如封源的群集，托管和支持。Prometheus 是一个完全开源的独立项目，由许多公司和个人维护，其中一些还提供商业服务和支持。\nPrometheus vs. OpenTSDB OpenTSDB is a distributed time series database based on Hadoop and HBase.\nScope The same scope differences as in the case of Graphite apply here.\nData model OpenTSDB’s data model is almost identical to Prometheus’s: time series are identified by a set of arbitrary key-value pairs (OpenTSDB tags are Prometheus labels). All data for a metric is stored together, limiting the cardinality of metrics. There are minor differences though: Prometheus allows arbitrary characters in label values, while OpenTSDB is more restrictive. OpenTSDB also lacks a full query language, only allowing simple aggregation and math via its API.\nStorage OpenTSDB’s storage is implemented on top of Hadoop and HBase. This means that it is easy to scale OpenTSDB horizontally, but you have to accept the overall complexity of running a Hadoop/HBase cluster from the beginning.\nPrometheus will be simpler to run initially, but will require explicit sharding once the capacity of a single node is exceeded.\nSummary Prometheus offers a much richer query language, can handle higher cardinality metrics, and forms part of a complete monitoring system. If you’re already running Hadoop and value long term storage over these benefits, OpenTSDB is a good choice.\nPrometheus vs. Nagios Nagios is a monitoring system that originated in the 1990s as NetSaint.\nScope Nagios is primarily about alerting based on the exit codes of scripts. These are called “checks”. There is silencing of individual alerts, however no grouping, routing or deduplication.\nThere are a variety of plugins. For example, piping the few kilobytes of perfData plugins are allowed to return to a time series database such as Graphite or using NRPE to run checks on remote machines.\nData model Nagios is host-based. Each host can have one or more services and each service can perform one check.\nThere is no notion of labels or a query language.\nStorage Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation.\nArchitecture Nagios servers are standalone. All configuration of checks is via file.\nSummary Nagios is suitable for basic monitoring of small and/or static systems where blackbox probing is sufficient.\nIf you want to do whitebox monitoring, or have a dynamic or cloud based environment, then Prometheus is a good choice.\nPrometheus vs. Sensu Sensu is a composable monitoring pipeline that can reuse existing Nagios checks.\nScope The same general scope differences as in the case of Nagios apply here.\nThere is also a client socket permitting ad-hoc check results to be pushed into Sensu.\nData model Sensu has the same rough data model as Nagios.\nStorage Sensu uses Redis to persist monitoring data, including the Sensu client registry, check results, check execution history, and current event data.\nArchitecture Sensu has a number of components. It uses RabbitMQ as a transport, Redis for current state, and a separate server for processing and API access.\nAll components of a Sensu deployment (RabbitMQ, Redis, and Sensu Server/API) can be clustered for highly available and redundant configurations.\nSummary If you have an existing Nagios setup that you wish to scale as-is, or want to take advantage of the automatic registration feature of Sensu, then Sensu is a good choice.\nIf you want to do whitebox monitoring, or have a very dynamic or cloud based environment, then Prometheus is a good choice.\ncomparison to alternatives\n","categories":"","description":"","excerpt":"Prometheus vs. Graphite Scope Graphite 专注于成为具有查询语言和图形功能的被动时间序列数据库。 其他任 …","ref":"/docs/monitoring/prometheus/introduction/comparison-to-alternatives/","tags":"","title":"Comparison to Alternatives"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/monitoring/prometheus/","tags":"","title":"Prometheus"},{"body":"General 什么是 Prometheus？ Prometheus 是具有活跃生态系统的开源系统监视和警报工具包。请参阅 overview。\nPrometheus 与其他监控系统相比如何？ 请参阅 comparison 页面。\nPrometheus 有什么依赖性？ 主要的 Prometheus server 独立运行，没有外部依赖性。\n可以使 Prometheus 高度可用吗？ 是的，在两台或更多台单独的计算机上运行相同的 Prometheus server。相同的警报将由 Alertmanager 进行重复数据删除。\n为了 Alertmanager的高可用，您可以在 Mesh cluster 中运行多个实例，并将 Prometheus server 配置为向每个实例发送通知。\n有人告诉我 Prometheus “不能缩放”。 实际上，存在多种缩放和联合 Prometheus 的方法。阅读 Robust Perception 博客上的 Scaling and Federating Prometheus，以开始使用。\nPrometheus 用什么语言编写？ 大多数 Prometheus 组件都是用Go编写的。有些还用 Java、Python 和 Ruby 编写。\nPrometheus 功能、存储格式和 API 的稳定性如何？ All repositories in the Prometheus GitHub organization that have reached version 1.0.0 broadly follow semantic versioning. Breaking changes are indicated by increments of the major version. Exceptions are possible for experimental components, which are clearly marked as such in announcements.\nEven repositories that have not yet reached version 1.0.0 are, in general, quite stable. We aim for a proper release process and an eventual 1.0.0 release for each repository. In any case, breaking changes will be pointed out in release notes (marked by [CHANGE]) or communicated clearly for components that do not have formal releases yet.\n为什么要拉取而不是推送？ 通过 HTTP 拉取有许多优点：\n开发更改时，可以在笔记本电脑上运行监控。 您可以更轻松地判断目标是否已关闭。 您可以手动转到目标并使用Web浏览器检查其运行状况。 总体而言，我们认为拉取要比推送略好，但在考虑使用监控系统时，不应将其视为重点。\n对于必须推送的情况，我们提供了 Pushgateway。\n如何将日志输入 Prometheus？ Short answer: Don’t! Use something like the ELK stack instead.\nLonger answer: Prometheus is a system to collect and process metrics, not an event logging system. The Raintank blog post Logs and Metrics and Graphs, Oh My! provides more details about the differences between logs and metrics.\nIf you want to extract Prometheus metrics from application logs, Google’s mtail might be helpful.\n谁写了 Prometheus？ Prometheus was initially started privately by Matt T. Proud and Julius Volz. The majority of its initial development was sponsored by SoundCloud.\nIt’s now maintained and extended by a wide range of companies and individuals.\nPrometheus 使用什么许可证？ Prometheus is released under the Apache 2.0 license.\nPrometheus 的复数是什么？ After extensive research, it has been determined that the correct plural of ‘Prometheus’ is ‘Prometheis’.\n我可以重新载入 Prometheus 的配置吗？ Yes, sending SIGHUP to the Prometheus process or an HTTP POST request to the /-/reload endpoint will reload and apply the configuration file. The various components attempt to handle failing changes gracefully.\n我可以发送警报吗？ Yes, with the Alertmanager.\nCurrently, the following external systems are supported:\nEmail Generic Webhooks HipChat OpsGenie PagerDuty Pushover Slack Can I create dashboards? Yes, we recommend Grafana for production usage. There are also Console templates.\nCan I change the timezone? Why is everything in UTC? To avoid any kind of timezone confusion, especially when the so-called daylight saving time is involved, we decided to exclusively use Unix time internally and UTC for display purposes in all components of Prometheus. A carefully done timezone selection could be introduced into the UI. Contributions are welcome. See issue #500 for the current state of this effort.\nInstrumentation Which languages have instrumentation libraries? There are a number of client libraries for instrumenting your services with Prometheus metrics. See the client libraries documentation for details.\nIf you are interested in contributing a client library for a new language, see the exposition formats.\nCan I monitor machines? Yes, the Node Exporter exposes an extensive set of machine-level metrics on Linux and other Unix systems such as CPU usage, memory, disk utilization, filesystem fullness, and network bandwidth.\nCan I monitor network devices? Yes, the SNMP Exporter allows monitoring of devices that support SNMP.\nCan I monitor batch jobs? Yes, using the Pushgateway. See also the best practices for monitoring batch jobs.\nWhat applications can Prometheus monitor out of the box? See the list of exporters and integrations.\nCan I monitor JVM applications via JMX? Yes, for applications that you cannot instrument directly with the Java client, you can use the JMX Exporter either standalone or as a Java Agent.\nWhat is the performance impact of instrumentation? Performance across client libraries and languages may vary. For Java, benchmarks indicate that incrementing a counter/gauge with the Java client will take 12-17ns, depending on contention. This is negligible for all but the most latency-critical code.\n故障排除 我的 Prometheus 1.x server 需要很长时间才能启动，并且会向日志中发送有关崩溃恢复的大量信息的垃圾邮件。 You are suffering from an unclean shutdown. Prometheus has to shut down cleanly after a SIGTERM, which might take a while for heavily used servers. If the server crashes or is killed hard (e.g. OOM kill by the kernel or your runlevel system got impatient while waiting for Prometheus to shutdown), a crash recovery has to be performed, which should take less than a minute under normal circumstances, but can take quite long under certain circumstances. See crash recovery for details.\nMy Prometheus 1.x server runs out of memory. See the section about memory usage to configure Prometheus for the amount of memory you have available.\nMy Prometheus 1.x server reports to be in “rushed mode” or that “storage needs throttling”. Your storage is under heavy load. Read the section about configuring the local storage to find out how you can tweak settings for better performance.\nImplementation Why are all sample values 64-bit floats? I want integers. We restrained ourselves to 64-bit floats to simplify the design. The IEEE 754 double-precision binary floating-point format supports integer precision for values up to 253. Supporting native 64 bit integers would (only) help if you need integer precision above 253 but below 263. In principle, support for different sample value types (including some kind of big integer, supporting even more than 64 bit) could be implemented, but it is not a priority right now. A counter, even if incremented one million times per second, will only run into precision issues after over 285 years.\nWhy don’t the Prometheus server components support TLS or authentication? Can I add those? Note: The Prometheus team has changed their stance on this during its development summit on August 11, 2018, and support for TLS and authentication in serving endpoints is now on the project’s roadmap. This document will be updated once code changes have been made.\nWhile TLS and authentication are frequently requested features, we have intentionally not implemented them in any of Prometheus’s server-side components. There are so many different options and parameters for both (10+ options for TLS alone) that we have decided to focus on building the best monitoring system possible rather than supporting fully generic TLS and authentication solutions in every server component.\nIf you need TLS or authentication, we recommend putting a reverse proxy in front of Prometheus. See, for example Adding Basic Auth to Prometheus with Nginx.\nThis applies only to inbound connections. Prometheus does support scraping TLS- and auth-enabled targets, and other Prometheus components that create outbound connections have similar support.\n","categories":"","description":"","excerpt":"General 什么是 Prometheus？ Prometheus 是具有活跃生态系统的开源系统监视和警报工具包。 …","ref":"/docs/monitoring/prometheus/introduction/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/envoy/","tags":"","title":"Envoy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/kubernetes/concepts/","tags":"","title":"Concepts"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/monitoring/","tags":"","title":"Monitoring"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/logging/","tags":"","title":"Logging"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cicd/","tags":"","title":"CI/CD"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/develop/","tags":"","title":"Develop"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/developer-tools/","tags":"","title":"Developer Tools"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/kubernetes/concepts/overview/","tags":"","title":"Overview"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/kubernetes/concepts/cluster-architecture/","tags":"","title":"Cluster Architecture"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/kubernetes/concepts/workloads/","tags":"","title":"Workloads"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/","tags":"","title":"Pods"},{"body":"Pod 是 Kubernetes 对象模型中最小的可部署对象。\n理解 Pods Pod 是 Kubernetes 应用程序的基本执行单元 - 在您创建或部署的 Kubernetes 对象模型中最小和最简单的单元。Pod 表示集群中运行的进程。\nPod 封装了一个应用程序的容器（或在某些情况下为多个容器），存储资源，唯一的网络标识（IP 地址）以及管理容器容器如何运行的选项。Pod 表示部署的单元：Kubernetes 中应用程序的单个实例，可能由单个容器或紧密耦合并共享资源的少量容器组成。\nKubernetes 集群中的 Pod 可以通过两种主要方式使用：\n运行单个容器的 Pod：”one-container-per-Pod“ 模型是最常见的 Kubernetes 用例。在这种情况下，您可以将 Pod 视为单个容器的包装，而 Kubernetes 则直接管理 Pod，而不是直接管理容器。 运行多个需要协同工作的容器的 Pod：Pod 可能封装了一个应用程序，该应用程序由紧密耦合且需要共享资源的多个并置容器组成。这些并置的容器可能形成一个内聚的服务单元 - 一个容器将文件从共享卷提供给公众，而一个单独的”sidecar“容器则刷新或更新这些文件。Pod 将这些容器和存储资源包装在一起，成为一个可管理的实体。 每个 Pod 旨在运行给定应用程序的单个实例。如果要水平扩展应用程序（通过运行更多实例来提供更多整体资源），则应使用多个 Pod，每个实例一个。在 Kubernetes 中，这通常称为复制 replication。复制的 Pods 通常作为一个组被 workload 资源及其 _controller_ 创建和管理。\nPods 如何管理多个容器 Pod 被设计为支持多个协作进程（即容器）组成一个内聚的服务单元。Pod 中的容器会自动地共同放置和调度到集群中的同一物理或虚拟机上。这些容器可以共享资源和依赖项，彼此通信，并协调何时以及如何终止他们。\n请注意，在单个 Pod 中对多个共同放置和管理的容器进行分组是一个相对高级的用例。您仅应在容器紧密耦合的特定实例中使用此模式。例如，您可能有一个容器充当共享卷中文件的 Web 服务器，以及一个单独的”sidecar“容器从远程源更新这些文件，如下图所示：\n有些 Pod 具有 init 容器和 app 容器。init 容器在 app 容器启动前运行并完成。\nPod 为其组成的容器提供两种共享资源：networking 和 storage。\nNetworking 每个 Pod 为每个地址族分配一个唯一的 IP 地址。Pod 中的每个容器都共享这个网络名称空间，包括 IP 地址和网络端口。在同一个 Pod 中的容器可以使用 localhost 相互通信。当 Pod 中的容器与 Pod 外部的实体进行通信时，它们必须协调如何使用共享的网络资源（例如端口）。\nStorage 一个 Pod 可以指定一组共享存储卷。这个 Pod 中的所有容器都可以访问共享卷，从而使这些容器可以共享数据。Volumes 还允许 Pod 中的持久化数据保存下来，以防其中的容器之一需要重新启动。\nWorking with Pods 您很少会直接在 Kubernetes 中创建单个 Pod – 甚至是单身 Pod。这是因为 Pod 被设计为相对短暂的一次性的实体。当一个 Pod 被创建时（直接由您创建，或者由 controller 间接创建），它将被安排在集群中的 Node 上运行。Pod 会保留在该节点上，直到进程终止，Pod 对象被删除，Pod 由于缺少资源而被驱逐，或节点发生故障为止。\n说明：不要将重新启动 Pod 中的容器与重新启动 Pod 混淆。Pod 不是进程，而是用于运行容器的环境。Pod 会一直存在直到被删除。\nPod 本身无法自我修复。如果 Pod 被调度到发生故障的节点，或者调度操作本身失败，Pod 将被删除。同样，由于缺乏资源或 Node 维护，Pod 无法幸免。Kubernetes 使用称为控制器的更高级的抽象来处理管理相对一次性的 Pod 实例的工作。因此，虽然可以直接使用 Pod，但在 Kubernetes 中使用控制器来管理 Pod 更为常见。\nPods and controllers 您可以使用工作负载资源为您创建和管理多个 Pod。资源的控制器处理 Pod 失败时的复制和回滚以及自动修复。例如，如果某个节点发生故障，则控制器会注意到该节点上的 Pod 已停止工作，并创建了一个替换 Pod。调度程序将替换的 Pod 放置到健康的节点上。\n以下是管理一个或多个 Pod 的工作负载资源的一些示例：\nDeployment StatefulSet DaemonSet Pod templates 工作负载资源的控制器从 Pod 模板创建 Pod，并代表您管理这些 Pod。 PodTemplates 是用于创建 Pod 的规范，并且包含在工作负载资源（如 Deployments，Jobs 和 DaemonSets）中。\n工作负载资源的每个控制器都使用工作负载对象内部的 PodTemplate 来创建实际的 Pod。PodTemplate 是用于运行应用程序的任何工作负载资源的期望状态的一部分。\n下面的示例是一个简单的 Job 的清单，带有一个启动一个容器的 template。该 Pod 中的容器会打印一条消息，然后暂停。\napiVersion: batch/v1 kind: Job metadata: name: hello spec: template: # This is the pod template spec: containers: - name: hello image: busybox command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" \u0026\u0026 sleep 3600'] restartPolicy: OnFailure # The pod template ends here 修改 pod 模板或切换到新的 pod 模板对已存在的 Pod 无效。 Pod 不会直接接收模板更新。而是创建一个新的 Pod 以匹配修订后的 Pod 模板。\n例如，Deployment 控制器可确保正在运行的 Pod 与当前 Pod 模板匹配。如果模板已更新，则控制器必须删除现有的 Pod 并根据更新的模板创建新的 Pod。每个工作负载控制器都实现自己的规则，以处理 Pod 模板的更改。\n在节点上，kubelet 不会直接观察或管理有关 pod 模板和更新的任何详细信息。这些细节被抽象掉了。关注点的抽象和分离简化了系统语义，并使得在不更改现有代码的情况下扩展集群的行为变得可行。\n","categories":"","description":"","excerpt":"Pod 是 Kubernetes 对象模型中最小的可部署对象。\n理解 Pods Pod 是 Kubernetes 应用程序的基本执行单元 - …","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/pod-overview/","tags":"","title":"Pod Overview"},{"body":"Pod 是可以在 Kubernetes 中创建和管理的最小的可部署计算单元。\nWhat is a Pod? Pod（就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个）容器（例如 Docker 容器），这些容器具有共享的存储/网络，以及有关如何运行容器的规范。Pod 的内容总是并置（co-located）的并且一同调度，在共享上下文中运行。Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个相对紧密耦合的应用容器 — 在容器出现之前，在相同的物理机或虚拟机上运行意味着在相同的逻辑主机上运行。\n虽然 Kubernetes 支持多种容器运行时，但 Docker 是最常见的一种运行时，它有助于使用 Docker 术语来描述 Pod。\nPod 的共享上下文是一组 Linux 命名空间、cgroups、以及其他潜在的资源隔离相关的因素，这些相同的东西也隔离了 Docker 容器。在 Pod 的上下文中，单个应用程序可能还会应用进一步的子隔离。\nPod 中的所有容器共享一个 IP 地址和端口空间，并且可以通过 localhost 互相发现。他们也能通过标准的进程间通信（如 SystemV 信号量或 POSIX 共享内存）方式进行互相通信。不同 Pod 中的容器的 IP 地址互不相同，没有 特殊配置 就不能使用 IPC 进行通信。这些容器之间经常通过 Pod IP 地址进行通信。\nPod 中的应用也能访问共享 卷，共享卷是 Pod 定义的一部分，可被用来挂载到每个应用的文件系统上。\n在 Docker 体系的术语中，Pod 被建模为一组具有共享命名空间和共享文件系统卷 的 Docker 容器。\n与单个应用程序容器一样，Pod 被认为是相对短暂的（而不是持久的）实体。如 Pod 的生命周期 所讨论的那样：Pod 被创建、给它指定一个唯一 ID（UID）、被调度到节点、在节点上存续直到终止（取决于重启策略）或被删除。如果 节点 宕机，调度到该节点上的 Pod 会在一个超时周期后被安排删除。给定 Pod （由 UID 定义）不会重新调度到新节点；相反，它会被一个完全相同的 Pod 替换掉，如果需要甚至连 Pod 名称都可以一样，除了 UID 是新的(更多信息请查阅 副本控制器（replication controller））。\n当某些东西被说成与 Pod（如卷）具有相同的生命周期时，这表明只要 Pod（具有该 UID）存在，它就存在。如果出于任何原因删除了该 Pod，即使创建了相同的 Pod，相关的内容（例如卷）也会被销毁并重新创建。\n一个多容器 Pod，其中包含一个文件拉取器和一个 Web 服务器，该 Web 服务器使用持久卷在容器之间共享存储\n设计 Pod 的目的 管理 Pod 是形成内聚服务单元的多个协作过程模式的模型。它们提供了一个比它们的应用组成集合更高级的抽象，从而简化了应用的部署和管理。Pod 可以用作部署、水平扩展和制作副本的最小单元。在 Pod 中，系统自动处理多个容器的在并置运行（协同调度）、生命期共享（例如，终止），协同复制、资源共享和依赖项管理。\n资源共享和通信 Pod 使它的组成容器间能够进行数据共享和通信。\nPod 中的应用都使用相同的网络命名空间（相同 IP 和 端口空间），而且能够互相“发现”并使用 localhost 进行通信。因此，在 Pod 中的应用必须协调它们的端口使用情况。每个 Pod 在扁平的共享网络空间中具有一个 IP 地址，该空间通过网络与其他物理计算机和 Pod 进行全面通信。\nPod 中的容器获取的系统主机名与为 Pod 配置的 name 相同。网络 部分提供了更多有关此内容的信息。\nPod 除了定义了 Pod 中运行的应用程序容器之外，Pod 还指定了一组共享存储卷。该共享存储卷能使数据在容器重新启动后继续保留，并能在 Pod 内的应用程序之间共享。\n使用 Pod Pod 可以用于托管垂直集成的应用程序栈（例如，LAMP），但最主要的目的是支持位于同一位置的、共同管理的工具程序，例如：\n内容管理系统、文件和数据加载器、本地缓存管理器等。 日志和检查点备份、压缩、旋转、快照等。 数据更改监视器、日志跟踪器、日志和监视适配器、事件发布器等。 代理、桥接器和适配器 控制器、管理器、配置器和更新器 通常，不会用单个 Pod 来运行同一应用程序的多个实例。\n有关详细说明，请参考 分布式系统工具包：组合容器的模式。\n可考虑的备选方案 为什么不在单个（Docker）容器中运行多个程序？\n透明度。Pod 内的容器对基础设施可见，使得基础设施能够向这些容器提供服务，例如进程管理和资源监控。这为用户提供了许多便利。 解耦软件依赖关系。可以独立地对单个容器进行版本控制、重新构建和重新部署。Kubernetes 有一天甚至可能支持单个容器的实时更新。 易用性。用户不需要运行他们自己的进程管理器、也不用担心信号和退出代码传播等。 效率。因为基础结构承担了更多的责任，所以容器可以变得更加轻量化。 为什么不支持基于亲和性的容器协同调度？\n这种处理方法尽管可以提供同址，但不能提供 Pod 的大部分好处，如资源共享、IPC、有保证的命运共享和简化的管理\nPod 的持久性（或稀缺性） 不得将 Pod 视为持久实体。它们无法在调度失败、节点故障或其他驱逐策略（例如由于缺乏资源或在节点维护的情况下）中生存。\n一般来说，用户不需要直接创建 Pod。他们几乎都是使用控制器进行创建，即使对于单例的 Pod 创建也一样使用控制器，例如 Deployments。控制器提供集群范围的自修复以及副本数和滚动管理。像 StatefulSet 这样的控制器还可以提供支持有状态的 Pod。\n在集群调度系统中，使用 API 合集作为面向用户的主要原语是比较常见的，包括 Borg、Marathon、Aurora、和 Tupperware。\nPod 暴露为原语是为了便于：\n调度器和控制器可插拔性 支持 Pod 级别的操作，而不需要通过控制器 API “代理” 它们 Pod 生命与控制器生命的解耦，如自举 控制器和服务的解耦 — 端点控制器只监视 Pod kubelet 级别的功能与集群级别功能的清晰组合 — kubelet 实际上是 “Pod 控制器” 高可用性应用程序期望在 Pod 终止之前并且肯定要在 Pod 被删除之前替换 Pod，例如在计划驱逐或镜像预先拉取的情况下。 Pod 的终止 因为 Pod 代表在集群中的节点上运行的进程，所以当不再需要这些进程时（与被 KILL 信号粗暴地杀死并且没有机会清理相比），允许这些进程优雅地终止是非常重要的。 用户应该能够请求删除并且知道进程何时终止，但是也能够确保删除最终完成。当用户请求删除 Pod 时，系统会记录在允许强制删除 Pod 之前所期望的宽限期，并向每个容器中的主进程发送 TERM 信号。一旦过了宽限期，KILL 信号就发送到这些进程，然后就从 API 服务器上删除 Pod。如果 Kubelet 或容器管理器在等待进程终止时发生重启，则终止操作将以完整的宽限期进行重试。\n流程示例：\n用户发送命令删除 Pod，使用的是默认的宽限期（30秒） API 服务器中的 Pod 会随着宽限期规定的时间进行更新，过了这个时间 Pod 就会被认为已 “死亡”。 当使用客户端命令查询 Pod 状态时，Pod 显示为 “Terminating”。 （和第 3 步同步进行）当 Kubelet 看到 Pod 由于步骤 2 中设置的时间而被标记为 terminating 状态时，它就开始执行关闭 Pod 流程。 如果 Pod 的容器之一定义了 preStop 钩子，就在容器内部调用它。如果宽限期结束了，但是 preStop 钩子还在运行，那么就用小的（2 秒）扩展宽限期调用步骤 2。 给 Pod 内的容器发送 TERM 信号。请注意，并不是所有 Pod 中的容器都会同时收到 TERM 信号，如果它们关闭的顺序很重要，则每个容器可能都需要一个 preStop 钩子。 （和第 3 步同步进行）从服务的端点列表中删除 Pod，Pod 也不再被视为副本控制器的运行状态的 Pod 集的一部分。因为负载均衡器（如服务代理）会将其从轮换中删除，所以缓慢关闭的 Pod 无法继续为流量提供服务。 当宽限期到期时，仍在 Pod 中运行的所有进程都会被 SIGKILL 信号杀死。 kubelet 将通过设置宽限期为 0 （立即删除）来完成在 API 服务器上删除 Pod 的操作。该 Pod 从 API 服务器中消失，并且在客户端中不再可见。 默认情况下，所有删除操作宽限期是 30 秒。kubectl delete 命令支持 --grace-period=\u003cseconds\u003e 选项，允许用户覆盖默认值并声明他们自己的宽限期。设置为 0 会强制删除 Pod。您必须指定一个附加标志 --force 和 --grace-period=0 才能执行强制删除操作。\nPod 的强制删除 强制删除 Pod 被定义为从集群状态与 etcd 中立即删除 Pod。当执行强制删除时，API 服务器并不会等待 kubelet 的确认信息，该 Pod 已在所运行的节点上被终止了。强制执行删除操作会从 API 服务器中立即清除 Pod， 因此可以用相同的名称创建一个新的 Pod。在节点上，设置为立即终止的 Pod 还是会在被强制删除前设置一个小的宽限期。\n强制删除对某些 Pod 可能具有潜在危险，因此应该谨慎地执行。对于 StatefulSet 管理的 Pod，请参考 从 StatefulSet 中删除 Pod 的任务文档。\nPod 容器的特权模式 Pod 中的任何容器都可以使用容器规范 security context 上的 privileged 参数启用特权模式。这对于想要使用 Linux 功能（如操纵网络堆栈和访问设备）的容器很有用。容器内的进程几乎可以获得与容器外的进程相同的特权。使用特权模式，将网络和卷插件编写为不需要编译到 kubelet 中的独立的 Pod 应该更容易。\n说明： 您的容器运行时必须支持特权容器模式才能使用此设置。\nAPI 对象 Pod 是 Kubernetes REST API 中的顶级资源。 Pod API 对象定义详细描述了该 Pod 对象。为 Pod 对象创建清单时，请确保指定的名称是有效的 DNS 子域名。\n","categories":"","description":"","excerpt":"Pod 是可以在 Kubernetes 中创建和管理的最小的可部署计算单元。\nWhat is a Pod? Pod（就像在鲸鱼荚或者豌豆荚 …","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/pods/","tags":"","title":"Pods"},{"body":"该页面将描述 Pod 的生命周期。\nPod phase Pod 的 status 定义在 PodStatus 对象中，其中有一个 phase 字段。\nPod 的运行阶段（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。\nPod 运行阶段值的数量和含义是严格指定的。除了本文档中列举的内容外，不应该再假定 Pod 有其他的 phase 值。\n下面是 phase 可能的值：\n挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。 Pod conditions Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。 PodCondition 数组的每个元素都有六个可能的字段：\nlastProbeTime 字段提供最后一次探测 Pod condition 的时间戳。 lastTransitionTime 字段提供 Pod 最后一次从一种状态过渡到另一种状态的时间戳。 message 字段是人类可读的消息，指示有关过渡的详细信息。 reason 字段是 condition 最后一次过渡的原因，该原因用唯一的、驼峰式的、一个词表示。 status 字段是一个字符串，可能的值有 “True\"、\"False” 和 “Unknown\"。 type 字段是一个字符串，具有以下可能的值： PodScheduled：已将Pod调度到一个节点； Ready：该 Pod 能够处理请求，应将其添加到所有匹配服务的负载均衡池中； Initialized：所有初始化容器已成功启动； ContainersReady：Pod 中的所有容器均已准备就绪。 容器探针 探针 是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 调用由容器实现的 Handler。有三种类型的处理程序：\nExecAction：在容器内执行指定命令。如果命令退出时状态码为 0 则认为诊断成功。 TCPSocketAction：对容器 IP 地址上指定的端口进行 TCP 检查。如果端口是开放的，则诊断被认为是成功的。 HTTPGetAction：对容器的 IP 地址上指定的端口和路径执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。 每次探测都将获得以下三种结果之一：\n成功（Success）：容器通过了诊断。 失败（Failure）：容器未通过诊断。 未知（Unknown）：诊断失败，因此不会采取任何行动。 kubelet 可以选择对正在运行的 Container 进行三种探针的执行并对其做出反应：\nlivenessProbe：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略 的影响。如果容器不提供存活探针，则默认状态为 Success。 readinessProbe：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success。 startupProbe: 指示容器中的应用是否已经启动。如果提供了启动探测（startup probe），则禁用所有其他探测，直到它成功为止。如果启动探测失败，kubelet 将杀死容器，容器服从其重启策略进行重启。如果容器没有提供启动探测，则默认状态为成功Success。 什么时候应该使用存活探针（livenessProbe）？ 功能状态：Kubernetes v1.0 [stable]\n如果容器中的进程能够在遇到问题或不健康的情况下能够自行崩溃，则不一定需要存活探针; kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作。\n如果您希望容器在探测失败时被杀死并重新启动，那么请指定一个存活探针，并指定restartPolicy 为 Always 或 OnFailure。\n什么时候应该使用就绪探针（readinessProbe）？ 功能状态：Kubernetes v1.0 [stable]\n如果您希望仅在探测成功时才开始向 Pod 发送流量，请指定就绪探针。在这种情况下，就绪探针可能与存活探针相同，但是 spec 中的就绪探针的存在意味着 Pod 将在没有接收到任何流量的情况下启动，并且只有在探针探测成功后才开始接收流量。如果您的容器需要在启动过程中加载大型数据、配置文件或迁移，请指定就绪探针。\n如果您希望容器能够自行维护，您可以指定一个就绪探针，该探针检查与存活探针不同的端点。\n请注意，如果您只是想在 Pod 被删除后能够排除请求，则不一定需要使用就绪探针；在删除 Pod 后，无论就绪探针是否存在，Pod 都会自动将自身置于未就绪状态。当等待 Pod 中的容器停止时，Pod 仍处于未完成状态。\n什么时候应该使用就绪探针（startupProbe）？ 功能状态：Kubernetes v1.16 [stable]\n如果您的容器通常在超过 initialDelaySeconds + failureThreshold × periodSeconds 的时间内启动，则应指定一个启动探针，该探针检查与存活探针相同的端点。periodSeconds 的默认值为 30s。然后，应将其 failureThreshold 设置得足够高，以允许 Container 启动，而不更改存活探针的默认值。这有助于防止死锁。\nPod 和容器状态 有关 Pod 容器状态的详细信息，请参阅 PodStatus 和 ContainerStatus。请注意，报告的 Pod 状态信息取决于当前的 ContainerState。\n容器状态 一旦 Pod 被调度器分配到节点后，kubelet 将开始使用容器运行时来创建容器。容器有三种可能的状态：Waiting、Running 和 Terminated。您可以使用 kubectl describe pod [POD_NAME] 来检查容器的状态。Pod 中每个容器的状态将会被显示。\nWaiting: 容器的默认状态。如果容器未处于 Running 或 Terminated 状态，则处于 Waiting 状态。处于 Waiting 状态的容器仍会运行其所需的操作，例如拉取镜像，应用秘密等。关于该状态的消息和原因会伴随着该状态显示，以提供更多的信息。\n... State: Waiting Reason: ErrImagePull ... Running: 表示容器正在执行，没有问题。 postStart 钩子（如果有）在容器进入 Running 状态之前执行。此状态还显示容器进入 Running 状态的时间。\n... State: Running Started: Wed, 30 Jan 2019 16:46:38 +0530 ... Terminated: 表示容器已完成其执行并已停止运行。当容器成功完成执行或由于某种原因失败时，容器就会进入该容器。无论如何，都会显示原因和退出代码，以及容器的开始和结束时间。在容器进入 Terminated 之前，执行 preStop 挂钩（如果有）。\n... State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 30 Jan 2019 11:45:26 +0530 Finished: Wed, 30 Jan 2019 11:45:26 +0530 ... Pod 就绪 功能状态：Kubernetes v1.14 [stable]\n您的应用程序可以向 PodStatus 注入额外的反馈或信号：Pod readiness。要使用此功能，请在 PodSpec 中设置 readinessGates 以指定 kubelet 为 Pod 就绪评估的其他条件列表。\nReadiness gates 由 Pod 的 status.condition 字段的当前状态决定。如果 Kubernetes 在 Pod 的 status.conditions 字段中找不到这样的条件，则该条件的状态默认为“False”。\n这是一个例子：\nkind: Pod ... spec: readinessGates: - conditionType: \"www.example.com/feature-1\" status: conditions: - type: Ready # a built in PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z - type: \"www.example.com/feature-1\" # an extra PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true ... 您添加的 Pod 条件必须具有符合Kubernetes label key format 的名称。\nPod 就绪状态 kubectl patch 命令不支持修补对象状态。要为 Pod 设置这些 status.conditions，应用程序和 操作员 应使用 PATCH 操作。您可以使用 Kubernetes client library 编写代码，来为 Pod 就绪设置自定义 Pod 条件。\n对于使用自定义条件的 Pod，仅当以下两个声明均适用时，该 Pod 才被评估为就绪：\nPod 中的所有容器均已准备就绪。 ReadinessGates 中指定的所有条件均为 True。 当 Pod 的容器准备就绪但至少一个自定义条件是缺失的或 False 时，kubelet 将 Pod 的条件设置为 ContainersReady。\n重启策略 PodSpec 中有一个 restartPolicy 字段，可能的值为 Always、OnFailure 和 Never。默认为 Always。 restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅指通过同一节点上的 kubelet 重新启动容器。失败的容器由 kubelet 以五分钟为上限的指数退避延迟（10秒，20秒，40秒…）重新启动，并在成功执行十分钟后重置。如 Pod 文档 中所述，一旦绑定到一个节点，Pod 将永远不会重新绑定到另一个节点。\nPod 的生命 一般来说，Pod 会一直保留，直到人为或 控制器 进程明确地销毁他们。当 Pod 的数量超过配置的阈值（取决于 kube-controller-manager 中的 terminate-pod-gc-threshold）时，控制平面将清理终止的 Pod（阶段为成功或失败）。这样可以避免在创建和终止 Pod 时资源泄漏。\n有多种创建Pod的资源：\n对不希望终止的 Pod 使用 Deployment、ReplicaSet 或 StatefulSet，例如 Web 服务器。 对希望在工作完成后终止的 Pod 使用 Job，例如批量计算。Job 仅适用于 restartPolicy 为 OnFailure 或 Never 的 Pod。 对需要在每个合格节点上运行一个的 Pod 使用 DaemonSet。 所有工作负载资源都包含一个 PodSpec。建议创建适当的工作负载资源，并让资源的控制器为您创建Pod，而不是自己直接创建Pods。\n如果某个节点死亡或与集群的其余部分断开连接，则 Kubernetes 将应用一个策略将丢失节点上的所有 Pod 的 phase 设置为 Failed。\n示例 高级 liveness 探针示例 存活探针由 kubelet 来执行，因此所有的请求都在 kubelet 的网络命名空间中进行。\napiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - args: - /server image: k8s.gcr.io/liveness livenessProbe: httpGet: # 当没有定义 \"host\" 时，使用 \"PodIP\" # host: my-host # 当没有定义 \"scheme\" 时，使用 \"HTTP\" scheme 只允许 \"HTTP\" 和 \"HTTPS\" # scheme: HTTPS path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 15 timeoutSeconds: 1 name: liveness 状态示例 Pod 中只有一个容器并且正在运行。容器成功退出。\n记录完成事件。\n如果 restartPolicy 为：\nAlways：重启容器；Pod phase 仍为 Running。 OnFailure：Pod phase 变成 Succeeded。\nNever：Pod phase 变成 Succeeded。 Pod 中只有一个容器并且正在运行。容器退出失败。\n记录失败事件。\n如果 restartPolicy 为：\nAlways：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。\nNever：Pod phase 变成 Failed。 Pod 中有两个容器并且正在运行。容器 1 退出失败。\n记录失败事件。\n如果 restartPolicy 为：\nAlways：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never：不重启容器；Pod phase 仍为 Running。 如果容器 1 没有处于运行状态，并且容器 2 退出：\n记录失败事件。\n如果 restartPolicy 为：\nAlways：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 - Never：Pod phase 变成 Failed。\nPod 中只有一个容器并处于运行状态。容器运行时内存超出限制：\n容器以失败状态终止。\n记录 OOM 事件。\n如果 restartPolicy 为：\nAlways：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。\nNever: 记录失败事件；Pod phase 变成 Failed。 Pod 正在运行，磁盘故障：\n杀掉所有容器。 记录适当事件。 Pod phase 变成 Failed。 如果使用控制器来运行，Pod 将在别处重建。 Pod 正在运行，其节点分段退出。\n节点控制器等待直到超时。 节点控制器将 Pod phase 设置为 Failed。 如果是用控制器来运行，Pod 将在别处重建。 ","categories":"","description":"","excerpt":"该页面将描述 Pod 的生命周期。\nPod phase Pod 的 status 定义在 PodStatus 对象中，其中有一个 phase …","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/pod-lifecycle/","tags":"","title":"Pod Lifecycle"},{"body":"本页提供了 Init 容器的概览：它是一种专用的容器，在 Pod 内的应用容器启动之前运行。Init 容器包括一些应用镜像中不存在的实用工具和安装脚本。\n你可以在 Pod 的规格信息中与 containers 数组（应用容器的描述）同级的位置指定 Init 容器。\n理解 Init 容器 Pod 可以包含多个容器，应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。\nInit 容器与普通容器完全一样，除了如下两点：\nInit 容器始终会运行到完成状态。 每个 init 容器必须成功完成才能启动下一个容器。 如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 值为 Never，它不会重新启动。\n为 Pod 指定 Init 容器，需要在 Pod 的 spec 中添加 initContainers 字段， 该字段內以Container 类型对象数组的形式组织，和应用的 containers 数组同级相邻。 Init 容器的状态在 status.initContainerStatuses 字段中以容器状态数组的格式返回（类似 status.containerStatuses 字段）。\n与普通容器的不同之处 Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。 然而，Init 容器对资源请求和限制的处理稍有不同，在下面 资源 处有说明。\n同时 Init 容器不支持 lifecycle、 livenessProbe、readinessProbe 或 startupProbe，因为它们必须在 Pod 就绪之前运行完成。\n如果为一个 Pod 指定了多个 Init 容器，kubelet 会按顺序逐个运行这些容器。每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。\n使用 Init 容器 因为 Init 容器具有与应用容器分离的单独镜像，因此他们在启动相关代码方面具有一些优势：\nInit 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。例如，没有必要仅为了在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具而去 FROM 一个镜像来生成一个新的镜像。 应用镜像的构建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。 Init 容器能以不同于Pod内应用容器的文件系统视图运行。因此，Init 容器可具有访问 Secrets 的权限，而应用容器不能够访问。 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。一旦前置条件满足，Pod内的所有的应用容器会并行启动。 Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。通过将不必要的工具分开，您可以限制应用容器镜像的攻击面。 示例 下面是一些如何使用 Init 容器的想法：\n等待一个 Service 完成创建，通过类似如下的单行 shell 命令：\nfor i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1 注册这个 Pod 到远程服务器，通过在命令中调用 API，类似如下：\ncurl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(\u003cPOD_NAME\u003e)\u0026ip=$(\u003cPOD_IP\u003e)' 在启动应用容器之前等一段时间，使用类似命令：\nsleep 60 克隆 Git 仓库到 Volume。\n将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。例如，在配置文件中存放 POD_IP 值，并使用 Jinja 生成主应用配置文件。\n使用 Init 容器 下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。第一个等待 myservice 启动，第二个等待 mydb 启动。 一旦这两个 Init容器 都启动完成，Pod 将启动 spec 区域中的应用容器。\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! \u0026\u0026 sleep 3600'] initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"] - name: init-mydb image: busybox:1.28 command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"] 您可以通过运行以下命令启动此Pod：\n$ kubectl apply -f myapp.yaml pod/myapp-pod created 并使用以下命令检查其状态：\n$ kubectl get -f myapp.yaml NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:0/2 0 6m 或了解更多详情：\n$ kubectl describe -f myapp.yaml Name: myapp-pod Namespace: default [...] Labels: app=myapp Status: Pending [...] Init Containers: init-myservice: [...] State: Running [...] init-mydb: [...] State: Waiting Reason: PodInitializing Ready: False [...] Containers: myapp-container: [...] State: Waiting Reason: PodInitializing Ready: False [...] Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container with docker id 5ced34a04634; Security:[seccomp=unconfined] 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container with docker id 5ced34a04634 如需查看 Pod 内 Init 容器的日志，请执行:\n$ kubectl logs myapp-pod -c init-myservice # Inspect the first init container $ kubectl logs myapp-pod -c init-mydb # Inspect the second init container 此时，Init 容器将会等待至发现名称为 mydb 和 myservice 的 Service。\n如下为创建这些 Service 的配置文件：\n--- apiVersion: v1 kind: Service metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 --- apiVersion: v1 kind: Service metadata: name: mydb spec: ports: - protocol: TCP port: 80 targetPort: 9377 创建 mydb 和 myservice service 的命令：\n$ kubectl create -f services.yaml service/myservice created service/mydb created 然后，您将能看到这些 Init 容器执行完毕，并且 my-app Pod 进入了 Running 状态：\n$ kubectl get -f myapp.yaml NAME READY STATUS RESTARTS AGE myapp-pod 1/1 Running 0 9m 这个简单的例子应该能为你创建自己的 Init 容器提供一些启发。 What’s next 部分提供了更详细例子的链接。\n具体行为 在 Pod 启动过程中，kubelet 会延迟运行 Init 容器，直到网络和存储就绪为止。然后，kubelet 按照 Pod 规范中出现的顺序运行 Pod 的 Init 容器。\n每个 Init 容器成功退出后才会启动下一个 Init容器。如果应为运行或退出时失败引发容器启动失败，它会根据 Pod 的 restartPolicy 策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 restartPolicy 的 OnFailure 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但会将条件 Initializing 设置为 true。\n如果 Pod 重启，所有 Init 容器必须重新执行。\n对 Init 容器 spec 的修改仅限于容器的 image 字段。更改 Init 容器的 image 字段，等同于重启该 Pod。\n因为 Init 容器可能会被重启、重试或者重新执行，所以 Init 容器的代码应该是幂等的。 特别地，基于 EmptyDirs 写文件的代码，应该对输出文件可能已经存在做好准备。\nInit 容器具有应用容器的所有字段。 然而 Kubernetes 禁止使用 readinessProbe，因为 Init 容器不能定义不同于完成（completion）的就绪（readiness）。 这一点会在校验时强制执行。\n在 Pod 上使用 activeDeadlineSeconds和在容器上使用 livenessProbe 可以避免 Init 容器一直重复失败。 activeDeadlineSeconds 时间包含了 Init 容器启动的时间。\n在 Pod 中的每个应用容器和 Init 容器的名称必须唯一；与任何其它容器共享同一个名称，会在校验时抛出错误。\n资源 给定 Init 容器的执行顺序下，资源使用适用于如下规则：\n所有 Init 容器上定义的对任何特定资源的 request 或 limit 的最大值，作为 有效的初始 request/limit Pod 对资源的有效 request/limit 是如下两者的较大者： 所有应用容器对某个资源的 request/limit 之和 对某个资源的有效初始 request/limit 基于有效 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源，这些资源在 Pod 生命周期过程中并没有被使用。 Pod 的 有效 QoS 层 ，与 Init 容器和应用容器的一样。 配额和限制将根据有效的 Pod request 和 limit 进行应用。\nPod 级别的 cgroups 是基于有效 Pod 的 request 和 limit，和调度器相同。\nPod 重启的原因 Pod 重启导致 Init 容器重新执行，主要有如下几个原因：\n用户更新 Pod 的 Spec 导致 Init 容器镜像发生改变。Init 容器镜像的变更会引起 Pod 重启. 应用容器镜像的变更仅会重启应用容器。 Pod 的基础设施容器（译者注：如 pause 容器）被重启。这种情况不多见，必须由具备Node root 权限访问的人员来完成。 当 restartPolicy 设置为 Always，Pod 中所有容器会终止而强制重启，由于垃圾收集导致 Init 容器的完成记录丢失。 ","categories":"","description":"","excerpt":"本页提供了 Init 容器的概览：它是一种专用的容器，在 Pod 内的应用容器启动之前运行。Init 容器包括一些应用镜像中不存在的实用工具 …","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/init-containers/","tags":"","title":"Init 容器"},{"body":"功能状态：Kubernetes v1.6 [alpha]\n本文提供了 PodPreset 的概述。在 Pod 创建时，用户可以使用 PodPreset 对象将特定信息注入 Pod 中，这些信息可以包括 secret、卷、卷挂载和环境变量。\n理解 Pod Preset Pod Preset 是一种 API 资源，在 Pod 创建时，用户可以用它将额外的运行时需求信息注入 Pod。 使用标签选择器（label selector）来指定 Pod Preset 所适用的 Pod。\n使用 Pod Preset 使得 Pod 模板编写者不必显式地为每个 Pod 设置所有信息。这样，使用特定服务的 Pod 模板编写者不需要了解该服务的所有细节。\n了解更多的相关背景信息，请参考 PodPreset 设计提案。\n在集群中启用 PodPreset 为了在集群中使用 Pod Preset，您必须确保以下几点：\n您已启用 API settings.k8s.io/v1alpha1/podpreset 类型。例如，可以通过在 API Server 的 --runtime-config 选项中包含 settings.k8s.io/v1alpha1=true 来实现。在使用 minikube 的情况下，在启动集群时添加此标志 --extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true。\n您已启用 PodPreset 准入控制器。一种方法是将 PodPreset 包含在为 API Server 指定的 --enable-admission-plugins 选项值中。在使用 minikube 的情况下，在启动集群时添加此标志 --extra-config=apiserver.enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset。\nPodPreset 如何工作 Kubernetes 提供了准入控制器（PodPreset），该控制器被启用时，会将 Pod Preset 应用于传入的 Pod 创建请求中。 当出现 Pod 创建请求时，系统会执行以下操作：\n检索所有可用 PodPresets 。 检查 PodPreset 的标签选择器与要创建的 Pod 的标签是否匹配。 尝试将 PodPreset 中定义的各种资源合并到正在创建的 Pod 中。 发生错误时，抛出一个事件用来记录在 Pod 上的合并错误，同时在 不注入 任何来自 PodPreset 的资源的情况下创建 Pod。 为修改后的 Pod spec 添加注解，来表明它已被 PodPreset 修改。注解的格式为： podpreset.admission.kubernetes.io/podpreset-\u003cpod-preset name\u003e\": \"\u003cresource version\u003e\"。 每个 Pod 可以匹配零个或多个 Pod Preset；并且每个 PodPreset 可以应用于零个或多个 Pod。当 PodPreset 应用于一个或多个 Pod 时，Kubernetes 会修改 Pod Spec。对于 Env、EnvFrom 和 VolumeMounts 的改动，Kubernetes 修改 Pod 中所有容器的规格；对于 Volume 的改动，Kubernetes 修改 Pod Spec。\n说明： 适当时候，Pod Preset 可以修改 Pod Spec 中的以下字段： - .spec.containers 字段 - initContainers 字段（需要 Kubernetes 1.14.0 或更高版本）。\n为特定 Pod 禁用 Pod Preset 在某些情况下，您希望 Pod 不受任何 Pod Preset 变动的影响。这时，您可以在 Pod Spec 中添加如下格式 podpreset.admission.kubernetes.io/exclude: \"true\" 的注解。\n","categories":"","description":"","excerpt":"功能状态：Kubernetes v1.6 [alpha]\n本文提供了 PodPreset 的概述。在 Pod 创建时， …","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/pod-preset/","tags":"","title":"Pod Preset"},{"body":"功能状态：Kubernetes v1.16 [alpha]\n您可以使用 拓扑扩展约束 来控制 Pod 在集群内故障域（例如地区，区域，节点和其他用户自定义拓扑域）之间的分布。这可以帮助实现高可用以及提升资源利用率。\n先决条件 启用功能 确保 EvenPodsSpread 功能已开启（在 1.16 版本中该功能默认关闭）。阅读功能选项了解如何开启该功能。EvenPodsSpread 必须在 API Server 和 scheduler 中都要开启。\n节点标签 拓扑扩展约束依赖于节点标签来标识每个节点所在的拓扑域。例如，一个节点可能具有标签：node=node1,zone=us-east-1a,region=us-east-1\n假设你拥有一个具有以下标签的 4 节点集群：\nNAME STATUS ROLES AGE VERSION LABELS node1 Ready \u003cnone\u003e 4m26s v1.16.0 node=node1,zone=zoneA node2 Ready \u003cnone\u003e 3m58s v1.16.0 node=node2,zone=zoneA node3 Ready \u003cnone\u003e 3m17s v1.16.0 node=node3,zone=zoneB node4 Ready \u003cnone\u003e 2m43s v1.16.0 node=node4,zone=zoneB 然后从逻辑上看集群如下：\n+---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ 可以复用在大多数集群上自动创建和填充的知名标签，而不是手动添加标签。\nPod 的拓扑约束 API 在 1.16 中引入的 pod.spec.topologySpreadConstraints 字段如下所示：\napiVersion: v1 kind: Pod metadata: name: mypod spec: topologySpreadConstraints: - maxSkew: \u003cinteger\u003e topologyKey: \u003cstring\u003e whenUnsatisfiable: \u003cstring\u003e labelSelector: \u003cobject\u003e 可以定义一个或多个 topologySpreadConstraint 来指示 kube-scheduler 如何将每个传入的 Pod 根据与现有的 Pod 的关联关系在集群中部署。字段包括：\nmaxSkew 描述 Pod 分布不均的程度。这是给定拓扑类型中任意两个拓扑域中匹配的 Pod 之间的最大允许差值。它必须大于零。 topologyKey 是节点标签的键。如果两个节点使用此键标记并且具有相同的标签值，则调度器会将这两个节点视为处于同一拓扑中。调度器试图在每个拓扑域中放置数量均衡的 Pod。 whenUnsatisfiable 指示如果 Pod 不满足扩展约束时如何处理： DoNotSchedule（默认）告诉调度器不用进行调度。 ScheduleAnyway 告诉调度器在对节点进行优先级排序以最大程度地减少偏斜的同时仍要调度它。 labelSelector 用于查找匹配的 Pod。匹配此标签的 Pod 将被统计，以确定相应拓扑域中 Pod 的数量。有关详细信息，请参考 Label Selectors。 您可以执行 kubectl explain Pod.spec.topologySpreadConstraints 命令了解更多关于 topologySpreadConstraints 的信息。\n示例：单个拓扑扩展约束 假设你拥有一个 4 节点集群，其中标记为 foo:bar 的 3 个 Pod 分别位于 node1，node2 和 node3 中（P 表示 pod）：\n+---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ | P | P | P | | +-------+-------+-------+-------+ 如果我们希望将传入的 Pod 与现有 Pod 均匀地分布在区域之间，则可以指定字段如下：\npods/topology-spread-constraints/one-constraint.yaml\nkind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 topologyKey: zone 意味着均匀分布将只应用于存在标签对为 “zone:” 的节点上。whenUnsatisfiable: DoNotSchedule 告诉调度器，如果传入的 Pod 不满足约束，则让它保持挂起状态。\n如果调度器将传入的 Pod 放入 “zoneA”，Pod 分布将变为 [3, 1]，因此实际的倾斜为 2（3 - 1）。这违反了 maxSkew: 1。此示例中，传入的 pod 只能放置在 “zoneB” 上：\n+---------------+---------------+ +---------------+---------------+ | zoneA | zoneB | | zoneA | zoneB | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | OR | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | P | P | P | P | | P | P | P P | | +-------+-------+-------+-------+ +-------+-------+-------+-------+ 您可以调整 Pod Spec 以满足各种要求：\n将 maxSkew 更改为更大的值，比如 “2”，这样传入的 Pod 也可以放在 “zoneA” 上。 将 topologyKey 更改为 “node”，以便将 Pod 均匀分布在节点上而不是区域中。在上面的例子中，如果 maxSkew 保持为 “1”，那么传入的 pod 只能放在 “node4” 上。 将 whenUnsatisfiable: DoNotSchedule 更改为 whenUnsatisfiable: ScheduleAnyway，以确保传入的 Pod 始终可以调度（假设满足其他的调度 API）。但是，最好将其放置在具有较少匹配 Pod 的拓扑域中。（请注意，此优先性与其他内部调度优先级（如资源使用率等）一起进行标准化。） 示例：多个拓扑扩展约束 下面的例子建立在前面例子的基础上。假设你拥有一个 4 节点集群，其中 3 个标记为 foo:bar 的 pod 分别位于 node1，node2 和 node3 上（P 表示 pod）：\n+---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ | P | P | P | | +-------+-------+-------+-------+ 可以使用 2 个拓扑扩展约束来控制 Pod 在 区域和节点两个维度上进行分布：\npods/topology-spread-constraints/two-constraints.yaml kind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar - maxSkew: 1 topologyKey: node whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 pods/topology-spread-constraints/two-constraints.yaml\nkind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar - maxSkew: 1 topologyKey: node whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 在这种情况下，为了匹配第一个约束，传入的 Pod 只能放置在 “zoneB” 中；而在第二个约束中，传入的 Pod 只能放置在 “node4” 上。然后两个约束的结果加在一起，因此唯一可行的选择是放置在 “node4” 上。\n多个约束可能导致冲突。假设有一个跨越 2 个区域的 3 节点集群：\n+---------------+-------+ | zoneA | zoneB | +-------+-------+-------+ | node1 | node2 | nod3 | +-------+-------+-------+ | P P | P | P P | +-------+-------+-------+ 如果对集群应用 “two-constraints.yaml”，会发现 “mypod” 处于 Pending 状态。这是因为：为了满足第一个约束，“mypod” 只能放在 “zoneB” 中，而第二个约束要求 “mypod” 只能放在 “node2” 上。Pod 调度无法满足两种约束。\n为了避免这种情况，您可以增加 maxSkew 或修改其中一个约束，让其使用 whenUnsatisfiable: ScheduleAnyway。\n约定 这里有一些值得注意的隐式约定：\n只有与传入 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。\n没有 topologySpreadConstraints[*].topologyKey 的节点将被忽略。这意味着：\n位于这些节点上的 Pod 不会影响 maxSkew 的计算。在上面的例子中，假设 “node1” 没有标签 “zone”，那么 2 个 Pod 将被忽略，因此传入的 Pod 将被调度到 “zoneA” 中。 传入的 Pod 没有机会被调度到这类节点上。在上面的例子中，假设一个带有标签 {zone-typo: zoneC} 的 “node5” 加入到集群，它将由于没有标签键 “zone” 而被忽略。 注意，如果传入 Pod 的 topologySpreadConstraints[*].labelSelector 与自身的标签不匹配，将会发生什么。在上面的例子中，如果移除传入 Pod 的标签，Pod 仍然可以调度到 “zoneB”，因为约束仍然满足。然而，在调度之后，集群的不平衡程度保持不变。zoneA 仍然有 2 个带有 {foo:bar} 标签的 Pod，zoneB 有 1 个带有 {foo:bar} 标签的 Pod。因此，如果这不是您所期望的，我们建议工作负载的 topologySpreadConstraints[*].labelSelector 与其自身的标签匹配。\n如果传入的 Pod 定义了 spec.nodeSelector 或 spec.affinity.nodeAffinity，则将忽略不匹配的节点。\n假设您有一个从 zoneA 到 zoneC 的 5 节点集群：\n+---------------+---------------+-------+ | zoneA | zoneB | zoneC | +-------+-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | node5 | +-------+-------+-------+-------+-------+ | P | P | P | | | +-------+-------+-------+-------+-------+ 并且您知道 “zoneC” 必须被排除在外。在这种情况下，可以按如下方式编写 yaml，以便将 “mypod” 放置在 “zoneB” 上，而不是 “zoneC” 上。同样，spec.nodeSelector 也要一样处理。\npods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml kind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: zone operator: NotIn values: - zoneC containers: - name: pause image: k8s.gcr.io/pause:3.1 pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml\nkind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: zone operator: NotIn values: - zoneC containers: - name: pause image: k8s.gcr.io/pause:3.1 集群级别的默认约束 功能状态：Kubernetes v1.18 [alpha]\n可以为集群设置默认拓扑扩展约束。仅在以下情况下，默认拓扑扩展约束将应用于Pod：\nPod 没有在 .spec.topologySpreadConstraints 中定义任何约束。 Pod 属于 Service、Replication Controller、ReplicaSet 或 StatefulSet。 可以在 调度配置文件 中将默认约束设置为 PodTopologySpread 插件 args 的一部分。约束使用与上面相同的 API 指定，但 labelSelector 必须为空。选择器是根据 Pod 所属的 Service、Replication Controller、ReplicaSet 或 StatefulSet 计算得出的。\n配置示例如下所示：\napiVersion: kubescheduler.config.k8s.io/v1alpha2 kind: KubeSchedulerConfiguration profiles: pluginConfig: - name: PodTopologySpread args: defaultConstraints: - maxSkew: 1 topologyKey: failure-domain.beta.kubernetes.io/zone whenUnsatisfiable: ScheduleAnyway 注意：默认调度约束产生的分数可能与 DefaultPodTopologySpread 插件 产生的分数冲突。当为 PodTopologySpread 使用默认约束时，建议您在调度配置文件中禁用此插件。\n与 PodAffinity/PodAntiAffinity 相比较 在 Kubernetes 中，与 “Affinity” 相关的指令控制 Pod 的调度方式（更密集或更分散）。\n对于 PodAffinity，可以尝试将任意数量的 Pod 打包到符合条件的拓扑域中。 对于 PodAntiAffinity，只能将一个 Pod 调度到单个拓扑域中。 “EvenPodsSpread” 功能提供了灵活的选项，用来将 Pod 均匀分布到不同的拓扑域中，以实现高可用性或节省成本。这也有助于滚动更新工作负载和平滑扩展副本。有关详细信息，请参考动机。\n已知局限性 1.18 版本（此功能为 Beta）存在如下已知限制：\nDeployment 的缩容可能导致 Pod 分布不平衡。 Pod 匹配到污点节点是允许的。参考 Issue 80921。 ","categories":"","description":"","excerpt":"功能状态：Kubernetes v1.16 [alpha]\n您可以使用 拓扑扩展约束 来控制 Pod 在集群内故障域（例如地区，区域，节点和 …","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/pod-topology-spread-constraints/","tags":"","title":"Pod 拓扑扩展约束"},{"body":"本指南针对的是希望构建高可用性应用程序的应用所有者，他们有必要了解可能发生在 Pod 上的干扰类型。\n文档同样适用于想要执行自动化集群操作（例如升级和自动扩展集群）的集群管理员。\n自愿干扰和非自愿干扰 Pod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。\n我们把这些不可避免的情况称为应用的非自愿干扰。例如：\n节点下层物理机的硬件故障 集群管理员错误地删除虚拟机（实例） 云提供商或虚拟机管理程序中的故障导致的虚拟机消失 内核错误 节点由于集群网络隔离从集群中消失 由于节点资源不足导致 Pod 被驱逐。 除了资源不足的情况，大多数用户应该都熟悉这些情况；它们不是特定于 Kubernetes 的。\n我们称其他情况为自愿干扰。包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者的 作包括：\n删除 deployment 或其他管理 pod 的控制器 更新了 deployment 的 pod 模板导致 pod 重启 直接删除 pod（例如，因为误操作） 集群管理员操作包括：\n排空（drain）节点进行修复或升级。 从集群中排空节点以缩小集群（了解集群自动扩缩）。 从节点中移除一个 pod，以允许其他 pod 使用该节点。 这些操作可能由集群管理员直接执行，也可能由集群管理员所使用的自动化工具执行，或者由集群托管提供商自动执行。\n咨询集群管理员或联系云提供商，或者查询发布文档，以确定是否为集群启用了任何自愿干扰源。如果没有启用，可以不用创建 Pod Disruption Budgets（Pod 干扰预算）。\n**警告：**并非所有的自愿干扰都会受到 Pod 干扰预算的限制。例如，删除 deployment 或 pod 的删除操作就会跳过 pod 干扰预算检查。\n处理干扰 以下是减轻非自愿干扰的一些方法：\n确保 Pod 请求所需资源。 如果需要更高的可用性，请复制应用程序。（了解有关运行多副本的无状态和有状态应用程序的信息。） 为了在运行复制应用程序时获得更高的可用性，请跨机架（使用反亲和性）或跨区域（如果使用多区域集群）扩展应用程序。 自愿干扰的频率各不相同。在一个基本的 Kubernetes 集群中，根本没有自愿干扰。然而，集群管理员或托管提供商可能运行一些可能导致自愿干扰的额外服务。例如，节点软件更新可能导致自愿干扰。另外，集群（节点）自动缩放的某些实现可能导致碎片整理和紧缩节点的自愿干扰。集群管理员或托管提供商应该已经记录了各级别的自愿干扰（如果有的话）。\nKubernetes 提供特性来满足在出现频繁自愿干扰的同时运行高可用的应用程序。我们称这些特性为干扰预算。\n干扰预算工作原理 功能状态： Kubernetes v1.5 [beta]\n应用程序所有者可以为每个应用程序创建 PodDisruptionBudget 对象（PDB）。PDB 将限制在同一时间因自愿干扰导致的复制应用程序中宕机的 Pod 数量。例如，基于仲裁的应用程序希望确保运行的副本数永远不会低于仲裁所需的数量。Web 前端可能希望确保提供负载的副本数量永远不会低于总数的某个百分比。\n集群管理员和托管提供商应该使用遵循 Pod Disruption Budgets 的工具（通过调用驱逐 API），而不是直接删除 Pod 或 Deployment。示例包括 kubectl drain 命令和 Kubernetes-on-GCE 集群升级脚本（cluster/gce/upgrade.sh）。\n当集群管理员想排空一个节点时，可以使用 kubectl drain 命令。该命令试图驱逐机器上的所有 Pod。驱逐请求可能会暂时被拒绝，且该工具定时重试失败的请求直到所有的 Pod 都被终止，或者达到配置的超时时间。\nPDB 指定应用程序可以容忍的副本数量（相当于应该有多少副本）。例如，具有 .spec.replicas: 5 的 Deployment 在任何时间都应该有 5 个 pod。如果 PDB 允许其在某一时刻有 4 个副本，那么驱逐 API 将允许同一时刻仅有一个而不是两个 Pod 自愿干扰。\n使用标签选择器来指定构成应用程序的一组 Pod，这与应用程序的控制器（deployment，stateful-set 等）选择 Pod 的逻辑一样。\nPod 控制器的 .spec.replicas 计算“预期的” Pod 数量。根据 Pod 对象的 .metadata.ownerReferences 字段来发现控制器。\nPDB 不能阻止非自愿干扰的发生，但是确实会计入预算。\n由于应用程序的滚动升级而被删除或不可用的 Pod 确实会计入干扰预算，但是控制器（如 deployment 和 stateful-set）在进行滚动升级时不受 PDB 的限制。应用程序更新期间的故障处理是在控制器的 spec 中配置的。（了解更新 deployment。）\n当使用驱逐 API 驱逐 Pod 时，Pod 会被优雅地终止（参考 PodSpec 中的 terminationGracePeriodSeconds）。\nPDB 例子 假设集群有 3 个节点，node-1 到 node-3。集群上运行了一些应用。其中一个应用有 3 个副本，分别是 pod-a，pod-b 和 pod-c。另外，还有一个不带 PDB 的无关 pod pod-x 也同样显示。最初，所有的 pod 分布如下：\nnode-1 node-2 node-3 pod-a available pod-b available pod-c available pod-x available 3 个 pod 都是 deployment 的一部分，并且共同拥有同一个 PDB，要求 3 个 pod 中至少有 2 个 pod 始终处于可用状态。\n例如，假设集群管理员想要重启系统，升级内核版本来修复内核中的 bug。集群管理员首先使用 kubectl drain 命令尝试排空 node-1 节点。命令尝试驱逐 pod-a 和 pod-x。操作立即就成功了。两个 pod 同时进入 terminating 状态。这时的集群处于下面的状态：\nnode-1 draining node-2 node-3 pod-a terminating pod-b available pod-c available pod-x terminating Deployment 控制器观察到其中一个 pod 正在终止，因此它创建了一个替代 pod pod-d。由于 node-1 被封锁（cordon），pod-d 落在另一个节点上。同样其他控制器也创建了 pod-y 作为 pod-x 的替代品。\n（注意：对于 StatefulSet 来说，pod-a（也称为 pod-0）需要在替换 pod 创建之前完全终止，替代它的也称为 pod-0，但是具有不同的 UID。反之，样例也适用于 StatefulSet。）\n当前集群的状态如下：\nnode-1 draining node-2 node-3 pod-a terminating pod-b available pod-c available pod-x terminating pod-d starting pod-y 在某一时刻，pod 被终止，集群如下所示：\nnode-1 drained node-2 node-3 pod-b available pod-c available pod-d starting pod-y 此时，如果一个急躁的集群管理员试图排空（drain）node-2 或 node-3，drain 命令将被阻塞，因为对于 deployment 来说只有 2 个可用的 pod，并且它的 PDB 至少需要 2 个。经过一段时间，pod-d 变得可用。\n集群状态如下所示：\nnode-1 drained node-2 node-3 pod-b available pod-c available pod-d available pod-y 现在，集群管理员试图排空（drain）node-2。drain 命令将尝试按照某种顺序驱逐两个 pod，假设先是 pod-b，然后是 pod-d。命令成功驱逐 pod-b，但是当它尝试驱逐 pod-d 时将被拒绝，因为对于 deployment 来说只剩一个可用的 pod 了。\nDeployment 创建 pod-b 的替代 pod pod-e。因为集群中没有足够的资源来调度 pod-e，drain 命令再次阻塞。集群最终将是下面这种状态：\nnode-1 drained node-2 node-3 no node pod-b available pod-c available pod-e pending pod-d available pod-y 此时，集群管理员需要增加一个节点到集群中以继续升级操作。\n可以看到 Kubernetes 如何改变干扰发生的速率，根据：\n应用程序需要多少个副本 优雅关闭应用实例需要多长时间 启动应用新实例需要多长时间 控制器的类型 集群的资源容量 分离集群所有者和应用所有者角色 通常，将集群管理者和应用所有者视为彼此了解有限的独立角色是很有用的。这种责任分离在下面这些场景下是有意义的：\n当有许多应用程序团队共用一个 Kubernetes 集群，并且有自然的专业角色 当第三方工具或服务用于集群自动化管理 Pod 干扰预算通过在角色之间提供接口来支持这种分离。\n如果你的组织中没有这样的责任分离，则可能不需要使用 Pod 干扰预算。\n如何在集群上执行干扰操作 如果你是集群管理员，并且需要对集群中的所有节点执行干扰操作，例如节点或系统软件升级，则可以使用以下选项\n接受升级期间的停机时间。 故障转移到另一个完整的副本集群。 没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。 编写可容忍干扰的应用程序和使用 PDB。 不停机。 最小的资源重复。 允许更多的集群管理自动化。 编写可容忍干扰的应用程序是棘手的，但对于支持容忍自愿干扰所做的工作，和支持自动扩缩和容忍非自愿干扰所做的工作相比，有大量的重叠。 ","categories":"","description":"","excerpt":"本指南针对的是希望构建高可用性应用程序的应用所有者，他们有必要了解可能发生在 Pod 上的干扰类型。\n文档同样适用于想要执行自动化集群操作（ …","ref":"/docs/cloudnative/kubernetes/concepts/workloads/pods/disruptions/","tags":"","title":"干扰（Disruptions）"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Blogs"},{"body":"运行环境 CentOS 7.9.2009 Docker 19.03.15 命令片段 运行环境\ndocker run -it -d --privileged --name monitoring centos:7 /usr/sbin/init docker exec -it monitoring bash yum install -y bash-completion git vim-enhanced curl wget mkdir /data wget https://objects.githubusercontent.com/github-production-release-asset-2e65be/6838921/85b84831-f125-4491-bf9b-5928b5edae01?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220622%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20220622T083108Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=272353bb4e9e85227018b4cbb4511b36a4767948e5fe5d894bce2cb59d15cafd\u0026X-Amz-SignedHeaders=host\u0026actor_id=15220555\u0026key_id=0\u0026repo_id=6838921\u0026response-content-disposition=attachment%3B%20filename%3Dprometheus-2.36.2.linux-amd64.tar.gz\u0026response-content-type=application%2Foctet-stream https://github.com/prometheus/prometheus/releases/download/v2.36.2/prometheus-2.36.2.linux-amd64.tar.gz ","categories":"","description":"","excerpt":"运行环境 CentOS 7.9.2009 Docker 19.03.15 命令片段 运行环境\ndocker run -it -d …","ref":"/blog/2022/06/22/monitoring/","tags":"","title":"Monitoring"},{"body":"开发环境 step1. 安装 Go\n$ brew install go go: stable 1.18 (bottled), HEAD Open source programming language to build simple/reliable/efficient software https://go.dev/ $ go version go version go1.18 darwin/amd64 step2. 设置 Go env\n# GOPATH, GOROOT, GOBIN, GO111MODULE, GOPROXY, GOPRIVATE $ go env -w GO111MODULE=on $ go env -w GOPROXY=https://goproxy.cn,direct step3. 安装 VSCode 及 vscode-go 插件\nGo: Install/Update Tools https://marketplace.visualstudio.com/items?itemName=golang.Go\nstep4. 开始 Go 开发之旅\nGo 命令行 go mod\ngo mod tidy 升级依赖\ngo get -u github.com/spf13/cobra go get -u github.com/spf13/pflag go get go install go mod 项目开发 命令行工具 $ gh repo clone luohu1/cmdcli-go 交叉编译 Mac 下编译 Linux 和 Windows 64 位可执行程序\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o gitlab cmd/gitlab/main.go CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o appname.exe appname.go Linux 下编译 Mac 和 Windows 64 位可执行程序\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build export GITHUB_TOKEN=\"ghp_lRMj********************************\" ~/go/bin/goreleaser release --rm-dist ~/go/bin/goreleaser release --snapshot --rm-dist Crontab 参考文档 https://github.com/robfig/cron/ https://pkg.go.dev/github.com/robfig/cron/v3 https://github.com/golang-standards/project-layout https://juejin.cn/post/6855129007038726152#heading-0 https://static.kancloud.cn/lhj0702/sockstack_gin/1805357 https://goreleaser.com/quick-start/ https://goreleaser.com/ ","categories":"","description":"","excerpt":"开发环境 step1. 安装 Go\n$ brew install go go: stable 1.18 (bottled), HEAD …","ref":"/docs/develop/go/","tags":"","title":"Go"},{"body":"elastalert\ndingtalk 插件开发\nPython 3.8.10\ngh repo clone luohu1/elastalert-dingtalk-plugin code elastalert-dingtalk-plugin cd elastalert-dingtalk-plugin python3 -m venv --copies venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install \"setuptools\u003e=11.3\" python3 -m pip install \"elasticsearch\u003e=5.0.0\" python3 -m pip install \"elastalert==0.2.4\" pip freeze \u003e requirements.txt mkdir elastalert_modules cd elastalert_modules touch __init__.py touch my_alerts.py # k8s 启动 elasticsearch\u0026kibana $ kubectl port-forward svc/elasticsearch 9200:9200 $ kubectl port-forward svc/kibana 15601:5601 $ elastalert-create-index Elastic Version: 7.7.1 Reading Elastic 6 index mappings: Reading index mapping 'es_mappings/6/silence.json' Reading index mapping 'es_mappings/6/elastalert_status.json' Reading index mapping 'es_mappings/6/elastalert.json' Reading index mapping 'es_mappings/6/past_elastalert.json' Reading index mapping 'es_mappings/6/elastalert_error.json' New index elastalert_status created Done! $ elastalert-test-rule --config \u003cpath-to-config-file\u003e example_rules/example_frequency.yaml $ python -m elastalert.elastalert --verbose 0 rules loaded INFO:elastalert:Starting up INFO:elastalert:Disabled rules are: [] INFO:elastalert:Sleeping for 59.999748 seconds # 开发 DingtalkAlerter 代码 Dockerfile\ndocker run -it -d --name es alpine:3.14 $ kubectl exec -it -n monitoring elastalert-68cdf7fdc7-tjbvm -- sh alpine 1.14 内置 python 3.9 elastalert 不兼容\n查询索引列表\n示例文件 使用 configmap\nkubectl create configmap elastalert-config --from-file=config.yaml elasticsearch deploy.yaml\nkubectl port-forward svc/elasticsearch 9200:9200 kubectl port-forward svc/kibana 15601:5601 --- apiVersion: apps/v1 kind: Deployment metadata: name: elasticsearch labels: app: elasticsearch spec: replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch image: elasticsearch:7.7.1 ports: - containerPort: 9200 env: - name: \"discovery.type\" value: \"single-node\" - name: \"bootstrap.memory_lock\" value: \"true\" resources: {} --- apiVersion: v1 kind: Service metadata: name: elasticsearch spec: type: ClusterIP ports: - port: 9200 protocol: TCP name: http targetPort: 9200 selector: app: elasticsearch --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana labels: app: kibana spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: automountServiceAccountToken: true securityContext: fsGroup: 1000 containers: - name: kibana securityContext: capabilities: drop: - ALL runAsNonRoot: true runAsUser: 1000 image: \"kibana:7.7.1\" imagePullPolicy: \"IfNotPresent\" env: - name: ELASTICSEARCH_HOSTS value: \"http://elasticsearch:9200\" - name: SERVER_HOST value: \"0.0.0.0\" - name: NODE_OPTIONS value: --max-old-space-size=1800 ports: - containerPort: 5601 resources: limits: cpu: 1000m memory: 2Gi requests: cpu: 1000m memory: 2Gi --- apiVersion: v1 kind: Service metadata: name: kibana spec: type: ClusterIP ports: - port: 5601 protocol: TCP name: http targetPort: 5601 selector: app: kibana 参考链接 https://github.com/Yelp/elastalert.git https://elastalert.readthedocs.io/en/latest/ https://github.com/xuyaoqiang/elastalert-dingtalk-plugin.git https://zhuanlan.zhihu.com/p/386722918 https://open.dingtalk.com/document/robots/custom-robot-access ","categories":"","description":"","excerpt":"elastalert\ndingtalk 插件开发\nPython 3.8.10\ngh repo clone …","ref":"/docs/logging/log-pilot/","tags":"","title":"Log Pilot"},{"body":"Quick Start 开发环境 Hugo v0.99.1+extended Node.js v14.19.1 with npm 6.14.16 themes/docsy dependencies/v0.2.0 安装 Hugo $ brew install hugo $ hugo version hugo v0.99.1+extended darwin/amd64 BuildDate=unknown 开始写作 $ git clone https://github.com/luohu1/website.git $ cd website $ git submodule update --init --recursive $ npm install $ hugo new docs/developer-tools/hugo/_index.md $ hugo server visit http://127.0.0.1:1313\n","categories":"","description":"","excerpt":"Quick Start 开发环境 Hugo v0.99.1+extended Node.js v14.19.1 with npm …","ref":"/docs/developer-tools/hugo/quick-start/","tags":"","title":"Quick Start"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/developer-tools/hugo/","tags":"","title":"Hugo"},{"body":"介绍 本文档演示了模块内部简单 Go 软件包的开发，并介绍了 go 工具，这是获取、构建和安装 Go 模块、软件包和命令的标准方法。\n注意：本文档假定您使用的是 Go 1.13 或更高版本，并且未设置 GO111MODULE 环境变量。如果您正在寻找本文档的较早的 pre-modules 版本，它在这里存档。\n代码组织 Go 程序被组织到 package 中。package 是同一目录中一起编译的源文件的集合。在一个源文件中定义的函数、类型、变量和常量对于同一 package 中的所有其他源文件可见。\n一个存储库（repository）包含一个或多个模块（module）。模块 是一起发布的相关 Go 软件包的集合。Go 存储库通常仅包含一个模块，位于存储库的根目录。在那里名为 go.mod 的文件声明了模块路径：模块内所有软件包的导入路径前缀。该模块包含了在包含其 go.mod 文件的目录以及该目录的子目录中包含的软件包，直至包含另一个 go.mod 文件（如果有）的下一个子目录。\n请注意，在构建代码之前，无需将代码发布到远程存储库。可以在本地定义模块，而不必属于存储库。但是，像您有一天要发布的代码一样组织代码是一种好习惯。\n每个模块的路径不仅充当其软件包的导入路径前缀，而且还指示 go 命令应该在哪里下载它。例如，为了下载模块 golang.org/x/tools，go 命令将查询 https://golang.org/x/tools 所指示的存储库（此处有更多说明）。\n导入路径 是用于导入软件包的字符串。包的导入路径是其模块路径及其在模块中的子目录。例如，模块 github.com/google/go-cmp 包含了一个在目录 cmp/ 中的包。该软件包的导入路径为 github.com/google/go-cmp/cmp。标准库中的软件包没有模块路径前缀。\n您的第一个程序 要编译和运行一个简单的程序，首先选择一个模块路径（我们将使用 example.com/user/hello），然后创建一个 go.mod 文件声明该路径：\n$ mkdir hello # 或者，如果它已经在版本控制中，则将其克隆。 $ cd hello $ go mod init example.com/user/hello go: creating new go.mod: module example.com/user/hello $ cat go.mod module example.com/user/hello go 1.14 Go 源文件中的第一条语句必须是 package name。可执行命令必须始终使用 package main。\n接下来，在该目录中创建一个名为 hello.go 的文件，其中包含以下Go 代码：\npackage main import \"fmt\" func main() { fmt.Println(\"Hello, world.\") } 现在，您可以使用 go 工具构建并安装该程序：\n$ go install example.com/user/hello 此命令构建 hello 命令，生成一个可执行二进制文件。然后，将该二进制文件安装为 $HOME/go/bin/hello（或在Windows下为 %USERPROFILE%\\go\\bin\\hello.exe）。\n安装目录由 GOPATH 和 GOBIN 环境变量控制。如果设置了 GOBIN，则二进制文件将安装到该目录。如果设置了 GOPATH，二进制文件将安装到 GOPATH 列表中第一个目录的 bin 子目录中。否则，二进制文件将安装到默认 GOPATH 的 bin 子目录（$HOME/go 或 %USERPROFILE%\\go）。\n您可以使用 go env 命令为以后的 go 命令便捷地设置环境变量的默认值：\n$ go env -w GOBIN=/somewhere/else/bin 要取消设置先前由 go env -w 设置的变量，请使用 go env -u：\n$ go env -u GOBIN 类似 go install 之类的命令适用于包含当前工作目录的模块的上下文。如果工作目录不在 example.com/user/hello 模块内，则 go install 可能会失败。\n为了方便起见，go 命令接受相对于工作目录的路径，如果没有给出其他路径，则默认使用当前工作目录中的软件包。因此，在我们的工作目录中，以下命令都是等效的：\n$ go install example.com/user/hello $ go install . $ go install 接下来，让我们运行该程序以确保其正常工作。为了更加方便，我们将安装目录添加到 PATH 中，以使运行二进制文件变得容易：\n# Windows 用户应该参考 https://github.com/golang/go/wiki/SettingGOPATH # 来设置 ％PATH％。 $ export PATH=$PATH:$(dirname $(go list -f '{{.Target}}' .)) $ hello Hello, world. 如果您在使用源代码控制系统，那么现在正是初始化存储库，添加文件并提交第一个更改的好时机。同样，此步骤是可选的：您无需使用源代码控制来编写Go代码。\n$ git init Initialized empty Git repository in /home/user/hello/.git/ $ git add go.mod hello.go $ git commit -m \"initial commit\" [master (root-commit) 0b4507d] initial commit 1 file changed, 7 insertion(+) create mode 100644 go.mod hello.go go 命令通过请求相应的 HTTPS URL 并读取 HTML 响应中嵌入的元数据来查找包含给定模块路径的存储库（请参阅 go help importpath）。许多托管服务已经为包含 Go 代码的存储库提供了该元数据，因此使您的模块可供其他人使用的最简单方法通常是使其模块路径与存储库的URL相匹配。\n从您的模块导入包 让我们编写一个 morestrings 包，并在 hello 程序中使用它。首先，为名为 $HOME/hello/morestrings 的包创建一个目录，然后在该目录中创建一个名为 reverse.go 的文件，其中包含以下内容：\n// 包 morestrings 实现了额外的功能来操纵 UTF-8 // 编码的字符串，超出标准 \"strings\" 包中提供的字符串。 package morestrings // ReverseRunes 返回其参数字符串，从左向右以符文方向反转。 func ReverseRunes(s string) string { r := []rune(s) for i, j := 0, len(r)-1; i \u003c len(r)/2; i, j = i+1, j-1 { r[i], r[j] = r[j], r[i] } return string(r) } 因为我们的 ReverseRunes 函数以大写字母开头，所以它是 exported，并且可以在导入 morestrings 包的其他包中使用。\n让我们测试一下使用 go build 编译该软件包：\n$ cd $HOME/hello/morestrings $ go build 这不会产生输出文件。而是将已编译的程序包保存在本地构建缓存中。\n确认 morestrings 软件包已生成后，让我们从 hello 程序中使用它。为此，请修改原始的 $HOME/hello/hello.go 以使用 morestrings 包：\npackage main import ( \"fmt\" \"example.com/user/hello/morestrings\" ) func main() { fmt.Println(morestrings.ReverseRunes(\"!oG ,olleH\")) } 安装 hello 程序：\n$ go install example.com/user/hello 运行新版本的程序，您应该看到一条新的反向的消息：\n$ hello Hello, Go! 从远程模块导入包 导入路径可以描述如何使用版本控制系统（例如 Git 或 Mercurial）获取软件包源代码。go 工具使用此属性来自动从远程存储库获取软件包。例如，要在您的程序中使用 github.com/google/go-cmp/cmp：\npackage main import ( \"fmt\" \"example.com/user/hello/morestrings\" \"github.com/google/go-cmp/cmp\" ) func main() { fmt.Println(morestrings.ReverseRunes(\"!oG ,olleH\")) fmt.Println(cmp.Diff(\"Hello World\", \"Hello Go\")) } 当您运行诸如 go install、go build 或 go run 之类的命令时，go 命令将自动下载远程模块并将其版本记录在您的 go.mod 文件中：\n$ go install example.com/user/hello go: finding module for package github.com/google/go-cmp/cmp go: downloading github.com/google/go-cmp v0.4.0 go: found github.com/google/go-cmp/cmp in github.com/google/go-cmp v0.4.0 $ hello Hello, Go! string( - \"Hello World\", + \"Hello Go\", ) $ cat go.mod module example.com/user/hello go 1.14 require github.com/google/go-cmp v0.4.0 模块依赖项将自动下载到 GOPATH 环境变量指示的目录的 pkg/mod 子目录中。这些给定版本的模块的下载内容在 require 该版本的所有其他模块之间共享，因此 go 命令将这些文件和目录标记为只读。要删除所有下载的模块，可以通过给 go clean 传递 -modcache 标志：\n$ go clean -modcache 测试 Go 具有由 go test 命令和 testing 包组成的轻量级测试框架。\n通过创建一个名称以 _test.go 结尾的文件来编写测试，该文件包含名为 TestXXX 且具有 func (t *testing.T) 签名的函数。测试框架运行每个这样的函数；如果该函数调用了诸如 t.Error 或 t.Fail 之类的失败函数，则认为测试已失败。\n通过创建包含以下 Go 代码的文件 $HOME/hello/morestrings/reverse_test.go，将测试添加到 morestrings 包中。\npackage morestrings import \"testing\" func TestReverseRunes(t *testing.T) { cases := []struct { in, want string }{ {\"Hello, world\", \"dlrow ,olleH\"}, {\"Hello, 世界\", \"界世 ,olleH\"}, {\"\", \"\"}, } for _, c := range cases { got := ReverseRunes(c.in) if got != c.want { t.Errorf(\"ReverseRunes(%q) == %q, want %q\", c.in, got, c.want) } } } 然后使用 go test 运行测试：\n$ go test PASS ok example.com/user/morestrings 0.165s 运行 go help test 和查看 testing package documentation 以获取更多详细信息。\n下一步 订阅 golang-announce 邮件列表，以便在发布新的 Go 稳定版时收到通知。\n有关编写清晰，惯用的 Go 代码的提示，请参见 Effective Go。\n参加 A Tour of Go 以正确地学习 Go 语言。\n请访问文档页面，获取有关 Go 语言及其库和工具的一系列深入文章。\n获得帮助 要获得实时帮助，请在社区运行的 gophers Slack server 中询问有用的 gopher（在此处获取邀请）。\n用于讨论 Go 语言的官方邮件列表是 Go Nuts。\n使用 Go 问题跟踪器报告错误。\n","categories":"","description":"如何编写 Go 代码","excerpt":"如何编写 Go 代码","ref":"/blog/2020/07/28/how-to-write-go-code/","tags":"","title":"How to Write Go Code"},{"body":" 此文章为 CoreOS Blog：Introducing Operators 的译文。原文由 Brandon Philips 在 2016 年 11 月 3 日发表。文章内容已经过时，因此仅作为理解 Operator 的参考文章。\n网站可靠性工程师（SRE）是通过编写软件来操作应用程序的人。他们是工程师，是开发者，他们知道如何专门为特定应用程序域开发软件。所产生的软件已在其中编程了应用程序的操作领域知识。我们的团队一直在 Kubernetes 社区中忙于设计和实施此概念，以在 Kubernetes 上可靠地创建、配置和管理复杂的应用程序实例。\n我们称这种新的软件类别为操作员。Operator 是特定于应用程序的控制器，它扩展了 Kubernetes API 以代表 Kubernetes 用户创建、配置和管理复杂的有状态应用程序实例。它建立在 Kubernetes 基本资源和控制器概念的基础上，但包括领域或特定应用程序的知识，可自动执行常见任务。\n无状态容易，有状态很难 借助 Kubernetes，即可相对轻松地管理和扩展 Web 应用程序、移动后端和开箱即用的 API 服务。为什么呢？因为这些应用程序通常是无状态的，因此基本的 Kubernetes API（例如 Deployments）可以在没有其他知识的情况下进行扩展并从故障中恢复。\n更大的挑战是管理有状态的应用程序，例如数据库、缓存和监控系统。这些系统需要应用程序领域知识来正确扩展、升级和重新配置，同时防止数据丢失或不可用。我们希望将此特定于应用程序的操作知识编码到软件中，以利用强大的 Kubernetes 抽象来正确地运行和管理应用程序。\nOperator 是一种软件，它对该领域知识进行了编码，并通过第三方资源机制扩展 Kubernetes API，使用户能够创建、配置和管理应用程序。像 Kubernetes 的内置资源一样，Operator 不仅管理应用程序的单个实例，而且管理整个集群中的多个实例。\n为了在运行中代码演示 Operator 的概念，如今我们有两个具体的示例公开为了开源项目：\netcd Operator 创建、配置和管理 etcd 集群。etcd 是 CoreOS 引入的一种可靠的分布式键值存储，用于维持分布式系统中最关键的数据，并且是 Kubernetes 本身的主要配置数据存储。 Prometheus Operator 创建、配置和管理 Prometheus 监控实例。Prometheus 是功能强大的监控、指标和警报工具，并且是 CoreOS 团队支持的 Cloud Native Computing Foundation（CNCF）项目。 如何构建 Operator？ Operator 基于 Kubernetes 的两个核心概念：资源和控制器。例如，内置的 ReplicaSet 资源使用户可以设置要运行的 Pod 的期望数量，并且 Kubernetes 内部的控制器通过创建或删除正在运行的 Pod 来确保 ReplicaSet 资源中设置的期望状态保持为 true。Kubernetes 中有许多以这种方式工作的基本控制器和资源，其中包括 Services、Deployments 和 Daemon Sets。\n示例1a：单个 Pod 正在运行，并且用户将期望的 Pod 数量更新为3。\n示例1b：过了一会儿，Kubernetes 内部的控制器创建了新的 Pod 来满足用户的要求。\nOperator 以 Kubernetes 基本资源和控制器概念为基础，并添加了一组知识或配置，以使 Operator 可以执行常见的应用程序任务。例如，当手动扩展 etcd 集群时，用户必须执行几个步骤：为新的 etcd 成员创建 DNS 名称，启动新的 etcd 实例，然后使用 etcd 管理工具（etcdctl member add）来告知现有群集有关此新成员的信息。取而代之的是，用户可以使用 etcd Operator 将 etcd 群集大小字段简单地增加 1。\n示例2：备份由用户使用 kubectl 触发。\nOperator 可能处理的其他复杂管理任务的示例包括安全协调应用程序升级、配置到异地存储的备份、通过原生 Kubernetes API 进行服务发现、应用程序 TLS 证书配置和灾难恢复。\n如何创建 Operator？ Operator 本质上是特定于应用程序的，因此艰苦的工作是将所有应用程序操作领域知识编码为合理的配置资源和控制循环。在构建 Operator 时，我们发现了一些常见的模式，这些模式我们认为对任何应用程序都很重要的：\nOperator 应将其作为单个部署进行安装，例如 kubectl create -f https://coreos.com/operators/etcd/latest/deployment.yaml，并且安装后无需执行任何其他操作。 Operator 在安装到 Kubernetes 中时应创建新的第三方类型。用户将使用此第三方类型创建新的应用程序实例。 Operator 应尽可能利用诸如 Services 和 Replica Sets 之类的内置 Kubernetes 原语，以利用经过良好测试和理解的代码。 Operator 应向后兼容，并始终理解用户创建的资源的先前版本。 Operator 应该被设计为无论 Operator 是否已停止或删除，应用程序实例都可以继续运行而不会受到影响。 Operator 应使用户能够声明期望版本，并基于期望版本协调应用程序升级。不升级软件是操作 bug 和安全问题的常见根源，Operator 可以帮助用户更自信地解决此负担。 Operator 应使用 “Chaos Monkey” 测试套件进行测试，该套件模拟了 Pod、配置和网络的潜在故障。 Operator 发展方向 今天，CoreOS 推出的 etcd Operator 和 Prometheus Operator 展示了 Kubernetes 平台的强大功能。去年，我们与更广泛的 Kubernetes 社区一起工作，专注于使 Kubernetes 稳定、安全、易于管理和快速安装。\n现在，在奠定了 Kubernetes 的基础之后，我们的新重点是建立在之上的系统：扩展 Kubernetes 使其具有新功能的软件。我们设想了一个未来，用户将在其 Kubernetes 集群上安装 Postgres Operator、Cassandra Operator 或 Redis Operator，并操作这些程序的可伸缩实例，就像它们今天轻松部署其无状态 Web 应用程序的副本一样。\n要了解更多信息，请深入 GitHub 仓库，在我们的[社区频道上讨论，或者 11 月 8 日星期二在 KubeCon 上与 CoreOS 团队进行交流。不要错过太平洋时间 11 月 8 日星期二下午 5:25 的主题演讲，在这里我将介绍 Operator 和其他 Kubernetes 主题。\nFAQ Q：这与 StatefulSets（以前称为 PetSets）有何不同？\nA：StatefulSets 旨在为集群中的应用程序提供支持，这些应用程序需要集群为其提供“有状态资源”，例如静态 IP 和存储。需要这种更具有状态部署模型的应用程序仍需要 Operator 自动的根据故障，备份或重新配置采取行动。因此，需要这些部署属性的应用程序的 Operator 可以使用 StatefulSet，而不是利用 ReplicaSet 或 Deployment。\nQ：这与 Puppet 或 Chef 之类的配置管理有何不同？\nA：容器和 Kubernetes 是最大的差异，它使 Operator 成为可能。通过这两种技术，使用 Kubernetes API 部署新软件、协调分布式配置以及检查多主机系统状态，这是一致且容易的。Operator 以一种对应用程序使用者有用的方式将这些原语粘合在一起；它不仅涉及配置，还涉及整个实时应用程序状态。\nQ：这与 Helm 有何不同？\nA：Helm 是用于将多个 Kubernetes 资源打包到一个包中的工具。将多个应用程序打包在一起的概念和使用 Operator 主动管理应用程序是互补的。例如，traefik 是一个负载均衡器，可以将 etcd 用作其后端数据库。您可以创建一个 Helm Chart，将部署 traefik Deployment 和 etcd 集群实例放在一起。etcd 集群稍后将由 etcd Operator 进行部署和管理。\nQ：Kubernetes 新手该怎么办？这意味着什么？\nA：对于新用户来说，除了使他们部署复杂的应用程序（例如 etcd、Prometheus和未来的其他应用）更容易以外，这将不会改变任何东西，除非它使他们将来更容易部署诸如etcd，Prometheus等复杂的应用程序。我们建议的 Kubernetes 入门路径仍然是 minikube，kubectl run，然后也许开始使用 Prometheus Operator 来监控使用 kubectl run 部署的应用程序。\nQ：如今 etcd Operator 和 Prometheus Operator 的代码是可用的吗？\nA：是的！可以在 GitHub 上的 https://github.com/coreos/etcd-operator 和 https://github.com/coreos/prometheus-operator 上找到他们。\nQ：您是否有其他 Operator 的计划？\nA：是的，将来可能会这样。我们也希望看到社区也建立了新的 Operator。让我们知道您接下来还想看到其他哪些 Operator。\nQ：Operator 如何帮助保护集群？\nA：不升级软件是操作 bug 和安全问题的常见根源，Operator 可以帮助用户更自信地解决正确升级的负担。\nQ：Operator 可以帮助灾难恢复吗？\nA：Operator 可以轻松地定期备份应用程序状态并从备份中恢复以前的状态。我们希望该功能将成为 Operator 的常见功能，它使用户能够轻松地从备份部署新实例。\n","categories":"","description":"Introducing Operators: Putting Operational Knowledge into Software.","excerpt":"Introducing Operators: Putting Operational Knowledge into Software.","ref":"/blog/2020/07/28/introducing-operators/","tags":"","title":"Introducing Operators"},{"body":" 此文章为 CoreOS Blog：The Prometheus Operator 的译文。原文由 Fabian Reinartz 在 2016 年 11 月 3 日发表。文章内容已经过时，因此仅作为理解 Operator 的参考文章。\n**Note: 这篇文章中的说明已过期。**要尝试 Prometheus Operator，请查看最新的 Prometheus 文档 ，以获取最新的入门指南。\n今天，CoreOS 推出了一种全新的软件类别，被称为 Operators，并且还将引入两个 Operator 作为开源项目，一个用于 etcd，另一个用于 Prometheus。在本文中，我们将概述 Operator 对于 Prometheus（Kubernetes 的监控系统）的重要性。\nOperator 以 Kubernetes 基本资源和控制器概念为基础，但包括应用程序领域知识以执行常见任务。它们最终将帮助您专注于期望的配置，而不是手动部署和生命周期管理的细节。\nPrometheus 是 Kubernetes 的近亲：Google 引入了 Kubernetes 作为其 Borg 集群系统的开源后代，Prometheus 来分享 Borgmon（与 Borg 配套的监控系统）的基本设计理念。如今，Prometheus 和 Kubernetes 都由 Cloud Native Computing Foundation（CNCF）管理。在技术层面上，Kubernetes 以原生 Prometheus 格式导出其所有内部指标。\nPrometheus Operator：整合 Kubernetes 和 Prometheus 的最佳方法 Prometheus Operator 只需一个命令行即可轻松安装，并且允许用户使用简单的声明性配置来配置和管理Prometheus实例，该配置将作为响应来创建，配置和管理Prometheus监视实例。\nPrometheus Operator 安装后，将提供以下功能：\n创建/销毁：使用 Operator 轻松为您的 Kubernetes 命名空间，特定的应用程序或团队启动 Prometheus 实例。\n简单配置：从原生 Kubernetes 资源配置 Prometheus 的基础知识，例如版本、持久性、保留策略和副本。\n通过标签的目标服务：基于熟悉的 Kubernetes 标签查询自动生成监控目标配置；无需学习 Prometheus 特定的配置语言。\n请注意，Prometheus Operator 正在大量开发中，请关注 GitHub上的项目 以获取最新信息。\n工作原理 Operator 的核心思想是将 Prometheus 实例的部署与它们所监视的实体的配置分离。为此，定义了两个第三方资源（TPR）：Prometheus 和 ServiceMonitor。\nOperator 始终确保对于群集中的每个 Prometheus 资源，一组具有期望配置的 Prometheus server 正在运行。这涉及到诸如数据保留时间、持久卷声明、副本数量、Prometheus 版本和 Alertmanager 实例向谁发送警报等方面。每个 Prometheus 实例都与各自的配置配对，该配置指定要向哪些监控目标抓取指标以及使用哪些参数。\n用户可以手动指定此配置，也可以让 Operator 根据第二个TPR ServiceMonitor 生成它。ServiceMonitor 资源指定如何从一组以通用方式公开指标的服务中检索指标。Prometheus 资源对象可以通过其标签动态包含 ServiceMonitor 对象。Operator 配置 Prometheus 实例去监控该实例中包括的 ServiceMonitor 所覆盖的所有服务，并使此配置与群集中发生的任何更改保持同步。\nOperator 封装了 Prometheus 领域知识的很大一部分，并且仅显示了对监控系统最终用户有意义的方面。这是一种强大的方法，可以使组织中所有团队的工程师以这种自主且灵活的方式运行他们的监控。\nPrometheus Operator in Action 我们将通过创建 Prometheus 实例和一些要监视的服务来逐步演示 Prometheus Operator。让我们从部署第一个 Prometheus 实例开始。\n首先，您需要一个运行中的 Kubernetes 集群，该集群版本为 v1.3.x 或 v1.4.x 并且启用了 alpha API（请注意，v1.5.0+ 的群集不适用于此博客文章中使用的 Prometheus Operator 版本；有关如何在新版本的 Kubernetes 上运行的最新信息，请参见Prometheus Operator 文档和 kube-prometheus）。如果您还没有一个集群，请按照 minikube 的提示快速启动并运行本地集群。\n注意：minikube 隐藏了 Kubernetes 的某些组件，但这是设置要使用的集群的最快方法。对于更广泛且类似于生产的环境，请查看使用 bootkube 设置集群。\n托管部署 让我们首先在集群中部署 Prometheus Operator：\n$ kubectl create -f https://coreos.com/operators/prometheus/latest/prometheus-operator.yaml deployment \"prometheus-operator\" created 验证它已启动并正在运行，并且已向 Kubernetes API server 注册了 TPR 类型。\n$ kubectl get pod NAME READY STATUS RESTARTS AGE prometheus-operator-1078305193-ca4vs 1/1 Running 0 5m $ until kubectl get prometheus; do sleep 1; done # … wait ... # If no more errors are printed, the TPR types were registered successfully. 部署单个 Prometheus 实例的 Prometheus TPR 的简单定义如下所示：\napiVersion: monitoring.coreos.com/v1alpha1 kind: Prometheus metadata: name: prometheus-k8s labels: prometheus: k8s spec: version: v1.3.0 要在集群中创建它，请运行：\n$ kubectl create -f https://coreos.com/operators/prometheus/latest/prometheus-k8s.yaml prometheus \"prometheus-k8s\" created service \"prometheus-k8s\" created 这还将创建服务以使用户可以访问 Prometheus UI。出于本演示的目的，创建了将其暴露在 NodePort 30900 上的服务。\n之后立即观察 Operator 部署 Prometheus pod：\n$ kubectl get pod -w NAME READY STATUS RESTARTS AGE prometheus-k8s-0 3/3 Running 0 2m 现在我们可以通过转到 http://:30900 来访问 Prometheus UI，使用 minikube 时运行 $ minikube service prometheus-k8s。\n以相同的方式，我们可以轻松地部署其他 Prometheus server，并在 Prometheus TPR 中使用高级选项，以使 Operator 能够处理版本升级、持久卷声明以及将 Prometheus 连接到 Alertmanager 实例。\n您可以在存储库文档中阅读有关托管 Prometheus 部署的全部功能的更多信息。\n集群监控 我们成功创建了托管的 Prometheus server。但是，由于我们未提供任何配置，因此它尚未监视任何内容。每个 Prometheus 部署都会挂载一个以自身命名的 Kubernetes ConfigMap，即我们的 Prometheus server 会在其命名空间中挂载 “prometheus-k8s” ConfigMap 中提供的配置。\n我们希望 Prometheus server 监视群集本身的所有方面，例如容器资源使用情况、群集节点和 kubelet。Kubernetes 选择 Prometheus 指标格式作为公开其所有组件的指标的方式。因此，我们只需要将 Prometheus 指向正确的端点即可检索这些指标。几乎在任何集群上这都可以工作，我们可以在 kube-prometheus 存储库中使用预定义的清单。\n# Deploy exporters providing metrics on cluster nodes and Kubernetes business logic $ kubectl create -f https://coreos.com/operators/prometheus/latest/exporters.yaml deployment \"kube-state-metrics\" created service \"kube-state-metrics\" created daemonset \"node-exporter\" created service \"node-exporter\" created # Create the ConfigMap containing the Prometheus configuration $ kubectl apply -f https://coreos.com/operators/prometheus/latest/prometheus-k8s-cm.yaml configmap \"prometheus-k8s\" configured Kubernetes 更新 Prometheus Pod 中配置后不久，我们可以在 “Targets” 页面上看到目标出现。Prometheus 实例现在正在接收指标，并已经可以在 UI 或仪表板中查询并评估警报。\n服务监控 除了监视集群组件之外，我们还希望监视我们自己的服务。使用常规的 Prometheus 配置，我们必须处理 relabeling 的概念才能正确发现和配置监视目标。它是一种强大的方法，可以使 Prometheus 与各种服务发现机制和任意操作模型集成。但是，它非常冗长和重复，因此通常不适合手动编写。\nPrometheus Operator 通过定义第二个 TPR 来解决此问题，该 TPR 表示如何以对 Kubernetes 完全惯用的方式监视我们的自定义服务。\n假设我们所有带有 tier = frontend 标签的服务都在命名端口 web 上的标准路径 /metrics 下提供了指标。ServiceMonitor TPR 允许我们声明性地表示适用于所有那些服务的监视配置，并通过标签 tier 进行选择。\napiVersion: monitoring.coreos.com/v1alpha1 kind: ServiceMonitor metadata: name: frontend labels: tier: frontend spec: selector: matchLabels: tier: frontend endpoints: - port: web # works for different port numbers as long as the name matches interval: 10s # scrape the endpoint every 10 seconds 这仅定义了应如何监视一组服务。我们现在需要定义 Prometheus 实例将该 ServiceMonitor 包含在其配置中。再次根据标签选择属于 Prometheus 设置的 ServiceMonitor。在部署所述 Prometheus 实例时，Operator 将根据匹配的服务监视器对其进行配置。\napiVersion: monitoring.coreos.com/v1alpha1 kind: Prometheus metadata: name: prometheus-frontend labels: prometheus: frontend spec: version: v1.3.0 # Define that all ServiceMonitor TPRs with the label `tier = frontend` should be included # into the server's configuration. serviceMonitors: - selector: matchLabels: tier: frontend 我们通过运行以下命令创建 ServiceMonitor 和 Prometheus 对象：\n$ kubectl create -f https://coreos.com/operators/prometheus/latest/servicemonitor-frontend.yaml servicemonitor \"frontend\" created $ kubectl create -f https://coreos.com/operators/prometheus/latest/prometheus-frontend.yaml prometheus \"prometheus-frontend\" created service \"prometheus-frontend\" created 访问 http://:30100（使用 minikube 时运行 $ minikube service prometheus-frontend），我们可以看到新的 Prometheus server 的UI。由于 ServiceMonitor 没有服务应用 ServiceMonitor，因此 “Targets” 页面仍然为空。\n以下命令部署四个示例应用程序的实例，以暴露由 ServiceMonitor 定义的指标，并匹配其 tier = frontend 标签选择器。\n$ kubectl create -f https://coreos.com/operators/prometheus/latest/example-app.yaml 回到 Web UI，我们可以看到新的 Pod 立即显示在 “Targets” 页面上，并且可以查询其公开的指标。我们的示例应用程序的服务和 Pod 标签，以及 Kubernetes 命名空间，作为标签自动地附加到了抓取的指标。这使我们能够在 Prometheus 查询和警报中进行汇总和过滤。\nPrometheus 将自动选择带有 tier = frontend 标签的新服务，并适配其上下扩展的部署。此外，如果添加、删除或修改 ServiceMonitor，Operator 将立即适当地重新配置 Prometheus。\n下图形象地显示了控制器如何通过观察我们的 Prometheus 和 ServiceMonitor 资源的状态来管理 Prometheus 部署。资源之间的关系通过标签表示，在运行时任何更改都会立即生效。\n未来发展方向 今天通过引入 Operator，我们展示了 Kubernetes 平台的强大功能。Prometheus Operator 扩展了 Kubernetes API 新的监视功能。我们已经了解了 Prometheus Operator 如何帮助我们动态部署 Prometheus 实例并管理其生命周期。此外，它提供了一种以纯粹的 Kubernetes 习惯用语表达定义服务监视的方法。监控真正成为集群本身的一部分，并且抽象出了所使用的不同系统的所有实现细节。\n尽管仍处于开发的早期阶段，但是 Operator 已经处理了 Prometheus 设置的多个方面，这些方面超出了本博客文章的范围，例如持久性存储、复制、警报和版本更新。查看 Operator 文档以了解更多信息。kube-prometheus 存储库包含各种基本知识，可以使您的群集监控立即启动并运行。它还为群集组件提供了现成的仪表板和警报。\n敬请期待 Prometheus Operator的更多功能和其他 operator 同样轻松地在集群内部运行 Prometheus Alertmanager 和 Grafana。\n在 KubeCon 上加入 CoreOS 2016 年 11 月 8 日至 9 日，我们将在西雅图 KubeCon 的 Kubernetes 会议上举办一系列活动。加入我们，尤其是在 11 月 9 日（星期三）下午 3:30 举行的普罗米修斯主题演讲中。 PT，它将深入探究 Prometheus Operator。\n确保检查出完整的 CoreOS KubeCon 活动日程，然后停下来，在 CoreOS 展位与我们的工程师一起解决您的 Kubernetes 和容器问题，或者请求与专家进行现场销售会议。\nBe sure to check out the full schedule of CoreOS KubeCon events, then stop by and visit our engineers at the CoreOS booth with your Kubernetes and container questions, or request an on-site sales meeting with a specialist.\n相关文章： Operator 简介：将操作知识纳入软件\n","categories":"","description":"The Prometheus Operator: Managed Prometheus setups for Kubernetes.","excerpt":"The Prometheus Operator: Managed Prometheus setups for Kubernetes.","ref":"/blog/2020/07/27/prometheus-operator/","tags":"","title":"Prometheus Operator"},{"body":"使用 Prometheus 监控 Kubernetes 集群\n前提条件 在开始之前，应确保以下条件是否满足：\nKubernetes 集群 安装 Prometheus 一个完备的 Prometheus 监控系统应该包含如下组件：\nPrometheus Server：收集并存储时间序列数据。 ","categories":"","description":"","excerpt":"使用 Prometheus 监控 Kubernetes 集群\n前提条件 在开始之前，应确保以下条件是否满足：\nKubernetes 集群 安 …","ref":"/blog/2020/07/25/prometheus/","tags":"","title":"Prometheus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cicd/jenkins/","tags":"","title":"Jenkins"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/cicd/jenkins/user-handbook/","tags":"","title":"User Handbook"},{"body":"在机器人技术和自动化领域, 控制回路是一个非终止回路，用于调节系统状态。\n控制回路的一个示例：房间中的恒温器\n当你设定温度时，即告诉恒温器你期望的状态 desired status。实际的室温是当前状态 current status。恒温器通过打开或关闭设备, 使当前状态更接近期望状态。\n在 Kubernetes 中，控制器是控制循环，它们监视集群的状态，然后在需要的时候进行更改或请求更改。每个控制器都尝试将当前集群状态移动到更接近期望状态。\nController pattern 一个控制器至少跟踪一种 Kubernetes 资源类型。这些对象有一个特定的字段代表着期望的状态。该资源的控制器负责使当前状态更接近于期望状态。\nControl via API server Job 控制器是 Kubernetes 内置控制器的一个示例。内置控制器通过与集群 API 服务器进行交互来管理状态。\nDirect control 于 Job 相比，某些控制器需要对集群外部的内容进行更改。\n与外部状态进行交互的控制器从 API 服务器中找到期望的状态，然后直接与外部系统进行通信以使当前状态更加紧密。\nDesired versus current status Design 作为其设计宗旨，Kubernetes 使用许多控制器，每个控制器管理集群状态的特定方面。最常见的是，一个特定的控制循环（controller）使用一种资源作为其期望状态，并有另一种不同的资源来设法使该期望状态发生。例如，Jobs 的控制器跟踪 Job 对象（以发现新工作）和 Pod 对象（以运行 Jobs，然后查看工作何时完成）。在这种情况下，其他东西创建 Jobs，而 job 控制器将创建 Pods。\nNote: 可以有多个控制器创建或更新相同类型的对象。在幕后，Kubernetes 控制器确保了他们仅关注与控制资源有关联的资源。例如，你可以有 Deployments 和 Jobs，他们都创建 Pods。Job 控制器不会删除 Deployment 创建的 Pods，因为控制器可以使用某些信息（标签）来区分这些 Pod。\nWays of running controllers Kubernetes 带有一组在 kube-controller-manager 内部运行的内置控制器。这些内置控制器提供了重要的核心行为。\nKubernetes 允许你运行弹性的控制平面，这样，如果任何内置控制器发生故障，控制平面的另一部分将接管工作。\n你可以找到在控制平面以外运行的控制器来扩展 Kubernetes。或者，如果需要，你可以编写一个新的控制器。你可以将自己的控制器作为一组 Pod 运行，也可以在 Kubernetes 外部运行。最合适的选择取决于特定控制器的功能。\n","categories":"","description":"","excerpt":"在机器人技术和自动化领域, 控制回路是一个非终止回路，用于调节系统状态。\n控制回路的一个示例：房间中的恒温器\n当你设定温度时，即告诉恒温器你 …","ref":"/docs/cloudnative/kubernetes/concepts/cluster-architecture/controllers/","tags":"","title":"Controllers"},{"body":"当你部署完 Kubernetes, 你就有了一个集群\nKubernetes 集群由一组工作机器组成, 这些工作机器称为节点 nodes, 运行容器化应用程序. 每个集群至少有一个工作节点.\n工作节点托管 Pods. 控制平面管理集群中的工作节点和 Pods.\nKubernetes 集群示意图\n控制平面组件(Control Plane Components) 控制平面组件可以运行在集群中的任何机器上. 然而, 为了简单, 设置脚本通常在同一台计算机上启动所有控制平面组件, 并且不在该计算机上运行用户容器.\nkube-apiserver 暴露 Kubernetes API\netcd 所有集群数据的后端存储\nkube-scheduler 监视新创建的还没有分配节点的Pod, 并选择一个节点使其运行\n调度决策要考虑的因素包括: 个人和集体资源需求, 硬件/软件/策略约束, 亲和力和反亲和力规范, 数据局限性, 工作负载之间的干扰以及期限.\nkube-controller-manager 运行控制器进程的控制平面组件\n从逻辑上讲, 每个控制器是一个单独的进程, 但是为了降低复杂性, 它们都被编译为单个二进制文件并在单个进程中运行.\nNode 控制器: 负责节点发生故障时的通知和响应 Replication 控制器: 负责为系统中的每个 replication 控制器对象维护正确数量的 Pods Endpoints 控制器: 填充 Endpoints 对象(加入 Services \u0026 Pods) Service Account \u0026 Token 控制器: 为新的名称空间创建默认账户和API访问令牌 cloud-controller-manager Node Components kubelet kubelet 获取一组 PodSpecs, 并确保这些 PodSpecs 中描述的容器是运行中和健康的\nkube-proxy kube-proxy 维护节点上的网络规则. 这些网络规则允许从集群内部或外部的网络会话于 Pod 进行网络通信\n如果有一个操作系统数据包过滤层可用, kube-proxy 使用操作系统数据包过滤层, 否则, kube-proxy 自己转发流量.\nContainer runtime Docker, containerd, CRI-O, 和 Kubernetes CRI 的任何实现\n插件(Addons) 插件使用 Kubernetes 资源实现集群功能. 因为它们提供集群级别的功能, 所以插件的命名空间资源属于kube-system 命名空间\n可用插件的扩展列表, 请参见插件(Addons)\nDNS 为 Kubernetes 服务提供 DNS 记录\nWeb UI (Dashboard) Dashboard 是 Kubernetes 集群的通用基于 Web 的 UI. 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除.\nContainner Resource Monitoring 容器资源监控将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中, 并提供用于浏览这些数据的界面\nCluster-level Logging 集群层面日志机制负责将容器的日志数据保存到一个集中的日志存储中, 该存储能够提供搜索和浏览接口.\n","categories":"","description":"","excerpt":"当你部署完 Kubernetes, 你就有了一个集群\nKubernetes 集群由一组工作机器组成, 这些工作机器称为节点 nodes, 运 …","ref":"/docs/cloudnative/kubernetes/concepts/overview/kubenetes-components/","tags":"","title":"Kubernetes Components"},{"body":"Kubernetes 名字起源于希腊语, 意思是舵手或飞行员\n历史回顾 传统部署时代 虚拟化部署时代 容器部署时代 容器带来的好处\n更快的应用创建和部署: 使用容器镜像相比使用 VM 镜像更加简单和高效. 持续开发, 集成和部署: 开发和运维的关注点分离: 可观察性不仅可以显示操作系统级别的信息和指标, 还可以显示应用的健康和其他信号. 环境一致性: 云和操作系统分发的可移植性 以应用程序为中心的管理: 松散耦合, 分布式, 弹性, 解放的微服务 资源隔离 资源利用率 为什么需要 Kubernetes 和 Kubernetes 可以做什么 Kubernetes 提供的能力\n服务发现和负载均衡 存储编排 自动部署和回滚 自动 bin packing 自我修复 Secret 和配置管理 What Kubernetes is not Kubernetes 不是一个传统的, 包罗万象的PaaS系统. 由于 Kubernetes 运行在容器层面而不是硬件层面, 因此它提供了 PaaS 产品所共有的一些普遍适用的功能, 例如部署, 扩展, 负载均衡, 并允许用户集成他们的日志记录, 监控和报警解决方案.\nKubernetes:\n不限制应用程序的类型: 无状态, 有状态和数据处理. 不部署源代码, 也不构建应用程序 不提供应用级别的服务作为内置服务, 例如中间件, 数据处理框架, 数据库, 缓存, 集群存储系统 不指示日志记录, 监控和报警解决方案 不提供也不要求配置语言/系统 不提供也不采用任何全面的机器配置, 维护, 管理或自我修复系统 Kubernetes 不仅仅是一个编排系统. 事实上, 它消除了编排的需要. 编排的技术定义是执行定义好的工作流程: 先做 A, 然后 B, 然后 C. 相反, Kubernetes 包含一组独立的, 可组合的控制进程, 这些控制过程连续地讲当前状态驱动到提供的期望状态. 它跟你如何从 A 到 C 的方式没有关系. 也不需要集中控制. ","categories":"","description":"","excerpt":"Kubernetes 名字起源于希腊语, 意思是舵手或飞行员\n历史回顾 传统部署时代 虚拟化部署时代 容器部署时代 容器带来的好处\n更快的应 …","ref":"/docs/cloudnative/kubernetes/concepts/overview/what-is-kubernetes/","tags":"","title":"What is Kubernetes?"},{"body":"安装 Jenkins 本节的步骤适用于在单台/本地计算机上的 Jenkins 新安装。\nJenkins 通常使用内置的 Java Servlet 容器/应用服务器（Jetty）在其自己的进程中作为独立应用程序运行。\nJenkins 也可以在其他 Java servlet 容器（例如 Apache Tomcat 或 GlassFish）中作为 servlet 运行。但是，设置这些类型的安装的说明超出了本页的范围。\n**注意：**尽管此页面侧重于 Jenkins 的本地安装，但此内容也可用于帮助在生产环境中设置 Jenkins。\n系统要求 最低硬件要求：\n256 MB 可用内存 1 GB 可用磁盘空间（如果将 Jenkins 作为 Docker 容器运行，建议最小为 10 GB） 为小团队推荐的硬件配置:\n1 GB + 可用内存 50 GB+ 可用磁盘空间 全面的硬件建议：\n硬件：请参阅 硬件建议 页面 软件要求：\nJava：请参阅 Java 要求 页面 Web 浏览器：请参阅 Web 浏览器兼容性 页面 对于 Windows 操作系统：Windows 支持策略 安装平台 本节介绍如何在不同的平台和操作系统上安装/运行 Jenkins。\nDocker Docker 是一个用于在称为“容器”（或 Docker 容器）的隔离环境中运行应用程序的平台。诸如 Jenkins 之类的应用程序可以作为只读“镜像”（或 Docker 镜像）下载，每个映像都作为容器在 Docker 中运行。Docker 容器实际上是 Docker 镜像的“运行实例”。从这个角度来看，镜像或多或少地被永久地存储（即：只要镜像更新发布），而容器被临时存储。在 Docker 文档的入门指南，Getting Started, Part 1: Orientation and setup 页面中阅读有关这些概念的更多信息。\nDocker 的基本平台和容器设计意味着可以在任何受支持的操作系统（macOS、Linux 和 Windows）或也运行着 Docker 的云服务（AWS 和 Azure）上运行单个 Docker 映像（对于任何给定的应用程序，如 Jenkins）。\n安装 Docker 要将 Docker 安装在您的操作系统上，请访问 Docker store 网站，然后单击适用于您的操作系统或云服务的 Docker Community Edition 框。按照其网站上的安装说明进行操作。\nJenkins 也可以在 Docker 企业版上运行，您可以通过 Docker 商店网站上的 Docker EE 进行访问。\n如果要在基于 Linux 的操作系统上安装 Docker，请确保配置 Docker，以便可以以非 root 用户身份对其进行管理。请在 Docker 文档的 Docker 的 Linux 安装后步骤 页面中阅读有关此内容的更多信息。该页面还包含有关如何配置 Docker 开机启动的信息。\n在 Docker 中下载并运行 Jenkins Jenkins 有多个可用的 Docker 镜像。\n推荐使用的 Docker 镜像是 jenkinsci/blueocean 镜像（来自 Docker Hub 存储库）。该镜像包含了与所有 Blue Ocean 插件和功能捆绑在一起的 Jenkins 的当前长期支持（LTS）版本（可以投入生产使用）。这意味着您不需要单独安装 Blue Ocean 插件。\n每次发布新版 Blue Ocean 时，都会发布新的 jenkinsci/blueocean 镜像。您可以在 tags 页面上看到 jenkinsci/blueocean 镜像以前发布的版本列表。\n您还可以使用其他 Jenkins Docker 映像（在 Docker Hub 上可通过 jenkins/jenkins 访问）。但是，这些不会随 Blue Ocean的发布而提供，需要通过 Jenkins 中的 Manage Jenkins \u003e Manage Plugins 页面进行安装。 在 Blue Ocean 入门 中了解更多信息。\n在 macOS 和 Linux 上 打开一个终端窗口。\n使用以下 docker network create 命令在 Docker 中创建 桥接网络：\ndocker network create jenkins 使用以下 docker volume create 命令创建以下存储卷，这些存储卷用来共享连接到 Docker 守护程序所需的 Docker 客户端 TLS 证书，和持久化 Jenkins 数据：\ndocker volume create jenkins-docker-certs docker volume create jenkins-data 为了在 Jenkins 节点内执行 Docker 命令，请使用以下 docker container run 命令下载并运行 docker:dind Docker 镜像：\ndocker container run \\ --name jenkins-docker \\ --rm \\ --detach \\ --privileged \\ --network jenkins \\ --network-alias docker \\ --env DOCKER_TLS_CERTDIR=/certs \\ --volume jenkins-docker-certs:/certs/client \\ --volume jenkins-data:/var/jenkins_home \\ --publish 2376:2376 \\ docker:dind 参数说明：\n--name jenkins-docker：（可选）指定用于运行镜像的 Docker 容器名称。默认情况下，Docker 将为容器生成一个唯一的名称。 --rm：（可选）关闭时自动删除 Docker 容器（Docker 镜像的实例）。它包含当被下面描述的 jenkinsci/blueocean 容器调用时 Docker 使用的 Docker 镜像缓存。 --detach：（可选）在后台运行 Docker 容器。此实例可以稍后通过运行 docker container stop jenkins-docker 停止，并可以通过 docker container start jenkins-docker 再次启动。有关更多容器管理命令，请参阅 docker container。 --privileged：当前在 Docker 中运行 Docker 需要特权访问才能正常运行。使用较新的 Linux 内核版本可以放宽此要求。 --network jenkins：这对应于先前步骤中创建的网络。 --network-alias docker：使 Docker 容器中的 Docker 在 jenkins 网络中作为主机名 docker 可用。 --env DOCKER_TLS_CERTDIR=/certs：在 Docker 服务器中启用 TLS 的使用。由于使用了特权容器，因此建议这样做，尽管它需要使用下面描述的共享存储卷。此环境变量控制被管理的 Docker TLS 证书的根目录。 --volume jenkins-docker-certs:/certs/client：将容器内的 /certs/client 目录映射到上面创建的名为 jenkins-docker-certs 的 Docker 存储卷。 --volume jenkins-data:/var/jenkins_home：将容器内的 /var/jenkins_home 目录映射到上面创建的名为 jenkins-data 的Docker 存储卷。这将允许此 Docker容器中的 Docker 守护程序控制的其他 Docker 容器从 Jenkins 装载数据。 --publish 2376:2376：（可选）公开主机上的 Docker 守护程序端口。这对于在主机上执行 docker 命令来控制此内部 Docker 守护程序很有用。 docker:dind：docker:dind 镜像本身。可以使用以下命令在运行之前下载此映像：docker image pull docker:dind。 下载 jenkinsci/blueocean 映像，并使用以下 docker container run 命令将其作为容器在 Docker 中运行：\ndocker container run \\ --name jenkins-blueocean \\ --rm \\ --detach \\ --network jenkins \\ --env DOCKER_HOST=tcp://docker:2376 \\ --env DOCKER_CERT_PATH=/certs/client \\ --env DOCKER_TLS_VERIFY=1 \\ --publish 8080:8080 \\ --publish 50000:50000 \\ --volume jenkins-data:/var/jenkins_home \\ --volume jenkins-docker-certs:/certs/client:ro \\ jenkinsci/blueocean 参数说明：\n--name jenkins-blueocean：（可选）为 jenkinsci/blueocean Docker 镜像的此实例指定 Docker 容器名称。这使得后续的 docker container 命令更易于引用它。 --rm：（可选）关闭时自动删除 Docker 容器（这是下面的 jenkinsci/blueocean 镜像的实例）。如果您需要退出 Jenkins，这可以保持整洁。 --detach：（可选）在后台运行 jenkinsci/blueocean 容器（即“detached”模式）并输出容器 ID。如果未指定此选项，则在终端窗口中输出此容器的运行中的 Docker 日志。 --network jenkins：将此容器连接到先前步骤中定义的 jenkins 网络。这使得上一步中的 Docker 守护程序可以通过主机名 docker 应用于此 Jenkins 容器。 --env DOCKER_HOST=tcp://docker:2376：指定 docker、 docker-compose 和其他 Docker 工具用于连接上一步中的 Docker 守护程序的环境变量。 --publish 8080:8080：将 jenkinsci/blueocean 容器的端口 8080映射（即“发布”）到主机上的端口 8080。第一个数字表示主机上的端口，而最后一个数字表示容器的端口。因此，如果为该选项指定 -p 49000:8080，则将通过主机上的端口 49000 访问 Jenkins。 --publish 50000:50000：（可选）将 jenkinsci/blueocean 容器的端口 50000 映射到主机上的端口 50000。仅当您在其他计算机上设置了一个或多个基于 JNLP 的 Jenkins 代理时才需要这样做，这些代理又与 jenkinsci/blueocean 容器（充当“master” Jenkins服务器，或仅充当“ Jenkins master服务器”）交互。基于 JNLP 的 Jenkins 代理默认情况下通过 TCP 端口 50000 与 Jenkins master 服务器通信。您可以通过 Configure Global Security 页面在 Jenkins master 服务器上更改此端口号。如果要将 JNLP 代理程序的 Jenkins master 服务器的 TCP 端口更改为 51000（例如），则需要重新运行 Jenkins（通过此 docker run … 命令），并使用以下命令指定此 “publish” 选项 --publish 52000:51000，其中最后一个值与 Jenkins 主服务器上更改后的值匹配，并且第一个值是 Jenkins master 服务器主机上的端口号，基于 JNLP 的 Jenkins 代理通过其通信（与 Jenkins master 服务器） - 即 52000。请注意，Jenkins 2.217 中的 WebSocket 代理不需要此配置。 --volume jenkins-data:/var/jenkins_home：将容器中的 /var/jenkins_home 目录映射到名称为 jenkins-data 的 Docker volume。除了将 /var/jenkins_home 目录映射到 Docker 存储卷之外，您还可以将此目录映射到计算机本地文件系统上的目录。例如，指定选项 --volume $HOME/jenkins:/var/jenkins_home 会将容器的 /var/jenkins_home 目录映射到本地计算机上 $HOME 目录中的 jenkins 子目录，该目录通常是 /Users/\u003cyour-username\u003e/jenkins 或者 /home/\u003cyour-username\u003e/jenkins。请注意，如果您为此更改源存储卷或目录，则需要更新上述 docker:dind 容器中的存储卷以匹配此卷。 --volume jenkins-docker-certs:/certs/client:ro：将 /certs/client 目录映射到先前创建的 jenkins-docker-certs 存储卷。这使连接到 Docker 守护程序所需的客户端 TLS 证书在 DOCKER_CERT_PATH 环境变量指定的路径中可用。 jenkinsci/blueocean：jenkinsci/blueocean Docker 镜像本身。如果尚未下载该镜像，则此 docker container run 命令将自动为您下载该镜像。此外，如果自上次运行此命令以来已发布了对该镜像的任何更新，则再次运行该命令将自动为您下载这些发布的镜像更新。**注意：**也可以使用 docker image pull 命令独立下载（或更新）此 Docker 镜像：docker image pull jenkinsci/blueocean 继续进行 Post-installation setup wizard。\n在 Windows 上 Jenkins 项目提供 Linux 容器镜像，而不是 Windows 容器镜像。确保将 Docker for Windows 安装配置为运行 Linux Containers 而不是 Windows Containers。请参阅 Docker 文档以获取 switch to Linux containers 的说明。配置为运行 Linux Containers 后，步骤如下：\n打开命令提示符窗口。\n使用以下 docker network create 命令在 Docker 中创建 桥接网络：\ndocker network create jenkins 使用以下 docker volume create 命令创建以下存储卷，这些存储卷用来共享连接到 Docker 守护程序所需的 Docker 客户端 TLS 证书，和持久化 Jenkins 数据：\ndocker volume create jenkins-docker-certs docker volume create jenkins-data 为了在 Jenkins 节点内执行 Docker 命令，请使用以下 docker container run 命令下载并运行 docker:dind Docker 镜像：\ndocker container run --name jenkins-docker --rm --detach ^ --privileged --network jenkins --network-alias docker ^ --env DOCKER_TLS_CERTDIR=/certs ^ --volume jenkins-docker-certs:/certs/client ^ --volume jenkins-data:/var/jenkins_home ^ docker:dind 下载 jenkinsci/blueocean 映像，并使用以下 docker container run 命令将其作为容器在 Docker 中运行：\ndocker container run --name jenkins-blueocean --rm --detach ^ --network jenkins --env DOCKER_HOST=tcp://docker:2376 ^ --env DOCKER_CERT_PATH=/certs/client --env DOCKER_TLS_VERIFY=1 ^ --volume jenkins-data:/var/jenkins_home ^ --volume jenkins-docker-certs:/certs/client:ro ^ --publish 8080:8080 --publish 50000:50000 jenkinsci/blueocean 有关每个选项的说明，请参阅上面的 macOS and Linux 说明。\n继续进行 Post-installation setup wizard。\n访问 Jenkins/Blue Ocean Docker 容器 如果您有使用 Docker 的经验，并且希望或需要使用 docker container exec 命令通过终端/命令提示符访问 jenkinsci/blueocean 容器，您可以添加 --name jenkins-blueocean 之类的选项（在上面的 docker container run 中），它将为 jenkinsci/blueocean 容器命名为 “jenkins-blueocean”。\n这意味着您可以使用如下类似的 docker container exec 命令（通过单独的终端/命令提示符窗口）访问容器：\ndocker container exec -it jenkins-blueocean bash 通过 Docker 日志访问 Jenkins 控制台日志 您可能需要访问 Jenkins 控制台日志。例如，在 Unlocking Jenkins 作为 Post-installation setup wizard 的一部分时。\n如果您没有在 上面 的 docker container run … 命令中指定分离模式选项 --detach，那么可以通过运行该 Docker 命令的终端/命令提示符窗口轻松访问 Jenkins 控制台日志。\n否则，您可以使用以下命令通过 jenkinsci/blueocean 容器的 Docker logs 访问 Jenkins 控制台日志：\ndocker container logs \u003cdocker-container-name\u003e 您的 \u003cdocker-container-name\u003e 可以使用 docker container ls 命令获取。如果您在上面的 docker container run … 命令中指定了 --name jenkins-blueocean 选项（另请参阅 访问 Jenkins/Blue Ocean Docker 容器），您可以简单地使用 docker container logs 命令：\ndocker container logs jenkins-blueocean 访问 Jenkins 家目录 您可能需要访问 Jenkins 家目录。例如，在 workspace 子目录中检查 Jenkins 构建的详细信息。\n如果您将 Jenkins 家目录（/var/jenkins_home）映射到计算机的本地文件系统上的目录（即，在 上面 的 docker container run … 命令），那么您可以通过计算机的常规终端/命令提示符访问该目录的内容。\n除此之外，如果您在 docker container run … 命令中指定了 --volume jenkins-data:/var/jenkins_home 选项，您可以使用 docker container exec 命令通过 jenkinsci/blueocean 容器的终端/命令提示符访问 Jenkins 家目录的内容：\ndocker container exec -it \u003cdocker-container-name\u003e bash 如上所述，您的 \u003cdocker-container-name\u003e 可以使用 docker container ls 命令获取。如果您在上面的 docker container run … 命令中指定了 --name jenkins-blueocean 选项（另请参阅 访问 Jenkins/Blue Ocean Docker 容器），您可以简单地使用 docker container exec 命令：\ndocker container exec -it jenkins-blueocean bash Post-installation setup wizard After downloading, installing and running Jenkins using one of the procedures above, the post-installation setup wizard begins.\nThis setup wizard takes you through a few quick “one-off” steps to unlock Jenkins, customize it with plugins and create the first administrator user through which you can continue accessing Jenkins.\nUnlocking Jenkins When you first access a new Jenkins instance, you are asked to unlock it using an automatically-generated password.\nBrowse to http://localhost:8080 (or whichever port you configured for Jenkins when installing it) and wait until the Unlock Jenkins page appears.\nFrom the Jenkins console log output, copy the automatically-generated alphanumeric password (between the 2 sets of asterisks).\nNote:\nThe command: sudo cat /var/lib/jenkins/secrets/initialAdminPassword will print the password at console. If you are running Jenkins in Docker using the official jenkins/jenkins image you can use sudo docker exec ${CONTAINER_ID or CONTAINER_NAME} cat /var/jenkins_home/secrets/initialAdminPassword to print the password in the console without having to exec into the container. On the Unlock Jenkins page, paste this password into the Administrator password field and click Continue. Notes:\nIf you ran Jenkins in Docker in detached mode, you can access the Jenkins console log from the Docker logs (above). The Jenkins console log indicates the location (in the Jenkins home directory) where this password can also be obtained. This password must be entered in the setup wizard on new Jenkins installations before you can access Jenkins’s main UI. This password also serves as the default admininstrator account’s password (with username “admin”) if you happen to skip the subsequent user-creation step in the setup wizard. Customizing Jenkins with plugins After unlocking Jenkins, the Customize Jenkins page appears. Here you can install any number of useful plugins as part of your initial setup.\nClick one of the two options shown:\nInstall suggested plugins - to install the recommended set of plugins, which are based on most common use cases. Select plugins to install - to choose which set of plugins to initially install. When you first access the plugin selection page, the suggested plugins are selected by default. If you are not sure what plugins you need, choose Install suggested plugins. You can install (or remove) additional Jenkins plugins at a later point in time via the Manage Jenkins \u003e Manage Plugins page in Jenkins.\nThe setup wizard shows the progression of Jenkins being configured and your chosen set of Jenkins plugins being installed. This process may take a few minutes.\nCreating the first administrator user Finally, after customizing Jenkins with plugins, Jenkins asks you to create your first administrator user.\nWhen the Create First Admin User page appears, specify the details for your administrator user in the respective fields and click Save and Finish. When the Jenkins is ready page appears, click Start using Jenkins. Notes: This page may indicate Jenkins is almost ready! instead and if so, click Restart. If the page does not automatically refresh after a minute, use your web browser to refresh the page manually. If required, log in to Jenkins with the credentials of the user you just created and you are ready to start using Jenkins! From this point on, the Jenkins UI is only accessible by providing valid username and password credentials.\nOffline Jenkins Installation This section describes how to install Jenkins on a machine that does not have an internet connection.\nTo install Jenkins itself, download the appropriate war file and transfer it to your machine.\nPlugins are a different matter, due to dependency requirements.\nThe recommended approach is to use Plugin Installation Manager Tool.\nIf you want to transfer the individual plugins, you’ll need to retrieve all dependencies as well. There are several dependency retrieval scripts and tools on Github. For example:\ninstall-plugins.sh - Bash script for managing plugins from the official Docker image for Jenkins samrocketman/jenkins-bootstrap-shared - Java is required; packages Jenkins and plugins into an immutable package installer. Supported formats include: RPM, DEB, Docker. Can proxy Jenkins and plugins through Nexus or Artifactory since Gradle is used to assemble plugins. Jenkins Parameters Jenkins initialization can also be controlled by run time parameters passed as arguments. Command line arguments can adjust networking, security, monitoring, and other settings.\nNetworking parameters Jenkins networking configuration is generally controlled by command line arguments. The networking configuration areguments are:\nCommand Line Parameter Description --httpPort=$HTTP_PORT Runs Jenkins listener on port $HTTP_PORT using standard http protocol. The default is port 8080. To disable (because you’re using https), use port -1. This option does not impact the root URL being generated within Jenkins logic (UI, JNLP files, etc.). It is defined by the Jenkins URL specified in the global configuration. --httpListenAddress=$HTTP_HOST Binds Jenkins to the IP address represented by $HTTP_HOST. The default is 0.0.0.0 — i.e. listening on all available interfaces. For example, to only listen for requests from localhost, you could use: --httpListenAddress=127.0.0.1 --httpsPort=$HTTPS_PORT Uses HTTPS protocol on port $HTTPS_PORT. This option does not impact the root URL being generated within Jenkins logic (UI, JNLP files, etc.). It is defined by the Jenkins URL specified in the global configuration. --httpsListenAddress=$HTTPS_HOST Binds Jenkins to listen for HTTPS requests on the IP address represented by $HTTPS_HOST. --http2Port=$HTTP_PORT Uses HTTP/2 protocol on port $HTTP_PORT. This option does not impact the root URL being generated within Jenkins logic (UI, JNLP files, etc.). It is defined by the Jenkins URL specified in the global configuration. --http2ListenAddress=$HTTPS_HOST Binds Jenkins to listen for HTTP/2 requests on the IP address represented by $HTTPS_HOST. --prefix=$PREFIX Runs Jenkins to include the $PREFIX at the end of the URL. For example, set –prefix=/jenkins to make Jenkins accessible at http://myServer:8080/jenkins --ajp13Port=$AJP_PORT Runs Jenkins listener on port $AJP_PORT using standard AJP13 protocol. The default is port 8009. To disable (because you’re using https), use port -1. --ajp13ListenAddress=$AJP_ADDR Binds Jenkins to the IP address represented by $AJP_HOST. The default is 0.0.0.0 — i.e. listening on all available interfaces. --sessionTimeout=$TIMEOUT Sets the http session timeout value to $SESSION_TIMEOUT minutes. Default to what webapp specifies, and then to 60 minutes Miscellaneous parameters Other Jenkins initialization configuration is also controlled by command line arguments. The miscellaneous configuration arguments are:\nCommand Line Parameter Description --argumentsRealm.passwd.$USER=$PASS Assigns the password for user $USER. If Jenkins security is enabled, you must log in as a user who has an admin role to configure Jenkins. --argumentsRealm.roles.$USER=admin Assigns user $USER the admin role. The user can configure Jenkins even if security is enabled in Jenkins. See Securing Jenkins for more information. --useJmx Enable Jetty Java Management Extension (JMX) Jenkins passes all command line parameters to the Winstone servlet container. More information about Jenkins Winstone command line parameters is available from the Winstone Command Line Parameter Reference.\nBe Careful with Command Line Parameters Jenkins ignores command line parameters it doesn’t understand instead of producing an error. Be careful when using command line parameters and make sure you have the correct spelling. For example, the parameter needed for defining the Jenkins administrative user is --argument**s**Realm and not --argumentRealm.\nJenkins properties Some Jenkins behaviors are configured with Java properties. Java properties are set from the command line that started Jenkins. Property assignments use the form -DsomeName=someValue to assign the value someValue to the property named someName. For example, to assign the value true to a property testName, the command line argument would be -DtestName=true.\nRefer to the detailed list of Jenkins properties for more information.\nConfiguring HTTP HTTPS with an existing certificate If you’re setting up Jenkins using the built-in Winstone server and want to use an existing certificate for HTTPS:\n--httpPort=-1 \\ --httpsPort=443 \\ --httpsKeyStore=path/to/keystore \\ --httpsKeyStorePassword=keystorePassword The keystore should be in JKS format (as created by the JDK ‘keytool’) and the keystore and target key must have the same password. (Placing the keystore arguments after Jenkins-specific parameters does not seem to work; either they are not forwarded to Winstone or Winstone ignores them coming after unknown parameters. So, make sure they are adjacent to the working --httpsPort argument.)\nIf your keystore contains multiple certificates (e.g. you are using CA signed certificate) Jenkins might end-up using a incorrect one. In this case you can convert the keystore to PEM and use following command line options:\n--httpPort=-1 \\ --httpsPort=443 \\ --httpsCertificate=path/to/cert \\ --httpsPrivateKey=path/to/privatekey Using HTTP/2 The HTTP/2 protocol allows web servers to reduce latency over encrypted connections by pipelining requests, multiplexing requests, and allowing servers to push in some cases before receiving a client request for the data. The Jetty server used by Jenkins supports HTTP/2 with the addition of the Application-Layer Protocol Negotiation (ALPN) TLS extension. The ALPN TLS extension is connected to the specific Jetty version and has specific requirements depending on the Java version.\nJava 11, Java 8u252, and later Java 11, Java 8 update 252 and Java 8 versions after update 252 can run the ALPN TLS extension by installing the Jetty ALPN java server jar and passing it as a java command line argument. Steps to install the extension are:\nIdentify the Jetty version included in your Jenkins server by searching the Jenkins startup log for the string org.eclipse.jetty.server.Server#doStart. For example: org.eclipse.jetty.server.Server#doStart: jetty-9.4.27.v20200227 Locate the Java version on the “System Information” page of “Manage Jenkins” to confirm it is Java 11 or 8u252 (or later) Download the jetty-alpn-java-server with the version number matching the Jetty version bundled with your Jenkins version Place the jetty-alpn-java-server.jar file in a directory accessible to the JVM Add --extraLibFolder=/path/to/extra/lib/folder to the Java command line arguments that start Jenkins java --extraLibFolder=/opt/java/jetty-alpn-java-server-9.4.27.v20200227.jar \\ -jar target/jenkins.war \\ --http2Port=9090 Java 8u242 and earlier Java 8 update 242 and earlier can run the ALPN TLS extension by installing the Jetty ALPN boot library corresponding to the exact OpenJDK version you are using into the Java boot classpath. Steps to install the extension are:\nIdentify the Java version running your Jenkins server from the “Manage Jenkins” → “System Information” page Find the boot library for your OpenJDK version Download the matching alpn-boot.jar file to a directory accessible to the JVM Add the alpn-boot.jar to the JVM boot classpath by adding -Xbootclasspath/p:/path/to/alpn-boot.jar to the Java command line arguments that start Jenkins java -Xbootclasspath/p:/opt/java/alpn-boot-8.1.13.v20181017.jar \\ -jar target/jenkins.war \\ --http2Port=9090 HTTPS certificates with Windows These instructions use a stock Jenkins installation on Windows Server. The instructions assume a certificate signed by a Certificate Authority such as Digicert. If you are making your own certificate skip steps 3, 4, and 5.\nThis process utilizes Java’s keytool. Use the Java keytool included with your Java installation.\nStep 1: Create a new keystore on your server. This will place a ‘keystore’ file in your current directory.\nC:\\\u003ekeytool -genkeypair -keysize 2048 -keyalg RSA -alias jenkins -keystore keystore Enter keystore password: Re-enter new password: What is your first and last name? [Unknown]: server.example.com What is the name of your organizational unit? [Unknown]: A Unit What is the name of your organization? [Unknown]: A Company What is the name of your City or Locality? [Unknown]: A City What is the name of your State or Province? [Unknown]: A State What is the two-letter country code for this unit? [Unknown]: US Is CN=server.example.com, OU=A Unit, O=A Company, L=A City, ST=A State, C=US correct? [no]: yes Enter key password for \u003cjenkins\u003e (RETURN if same as keystore password): Step 2: Verify the keystore was created (your fingerprint will vary)\nC:\\\u003ekeytool -list -keystore keystore Enter keystore password: Keystore type: JKS Keystore provider: SUN Your keystore contains 1 entry jenkins, May 6, 2015, PrivateKeyEntry, Certificate fingerprint (SHA1): AA:AA:AA:AA:AA:AA:AA:AA:AA:AA ... Step 3: Create the certificate request. This will create a ‘certreq.csr’ file in your current directory.\nC:\\\u003ekeytool -certreq -alias jenkins -keyalg RSA ^ -file certreq.csr ^ -ext SAN=dns:server-name,dns:server-name.your.company.com ^ -keystore keystore Enter keystore password: Step 4: Use the contents of the certreq.csr file to generate a certificate from your certificate provider. Request a SHA-1 certificate (SHA-2 is untested but will likely work). If using DigiCert, download the resulting certificate as Other format “a .p7b bundle of all the certs in a .p7b file”.\nStep 5: Add the resulting .p7b into the keystore you created above.\nC:\\\u003ekeytool -import ^ -alias jenkins ^ -trustcacerts ^ -file response_from_digicert.p7b ^ -keystore keystore Enter keystore password: Certificate reply was installed in keystore Step 6: Copy the ‘keystore’ file to your Jenkins secrets directory. On a stock installation, this will be at\nC:\\Program Files (x86)\\Jenkins\\secrets Step 7: Modify the section of your C:\\Program Files (x86)\\Jenkins\\jenkins.xml file to reflect the new certificate. Note: This example disables http via --httpPort=-1 and places the server on 8443 via --httpsPort=8443.\n\u003carguments\u003e -Xrs -Xmx256m -Dhudson.lifecycle=hudson.lifecycle.WindowsServiceLifecycle -jar \"%BASE%\\jenkins.war\" --httpPort=-1 --httpsPort=8443 --httpsKeyStore=\"%BASE%\\secrets\\keystore\" --httpsKeyStorePassword=your.password.here \u003c/arguments\u003e Step 8: Restart the jenkins service to initialize the new configuration.\nnet stop jenkins net start jenkins Step 9: After 30-60 seconds, Jenkins will have completed the startup process and you should be able to access the website at https://server.example.com:8443. Verify the certificate looks good via your browser’s tools. If the service terminates immediately, there’s an error somewhere in your configuration. Useful error information can be found in:\nC:\\Program Files (x86)\\Jenkins\\jenkins.err.log C:\\Program Files (x86)\\Jenkins\\jenkins.out.log ","categories":"","description":"","excerpt":"安装 Jenkins 本节的步骤适用于在单台/本地计算机上的 Jenkins 新安装。\nJenkins 通常使用内置的 Java …","ref":"/docs/cicd/jenkins/user-handbook/installing-jenkins/","tags":"","title":"Installing Jenkins"},{"body":"用户手册概述 此页面提供了《Jenkins 用户手册》中文档的概述。\n如果您想启动并运行 Jenkins，请参阅 安装 Jenkins，以获取有关如何在支持的所选平台上安装 Jenkins 的过程。\n如果您是典型的 Jenkins 用户（任何技能水平），并且想进一步了解 Jenkins 的用法，请参阅 使用 Jenkins。另请参阅 Pipeline 和 Blue Ocean 章节，以获有关这些 Jenkins 核心特性的更多信息。\n如果您是 Jenkins 管理员，并且想了解有关管理 Jenkins 节点和实例的更多信息，请参阅 管理 Jenkins。\n如果您是系统管理员，并且想学习如何备份、恢复和维护 Jenkins 服务器和节点，请参阅 Jenkins 系统管理。\n","categories":"","description":"","excerpt":"用户手册概述 此页面提供了《Jenkins 用户手册》中文档的概述。\n如果您想启动并运行 Jenkins，请参阅 安装 Jenkins，以获 …","ref":"/docs/cicd/jenkins/user-handbook/overview/","tags":"","title":"User Handbook Overview"},{"body":" LuoHui's website! Learn More Source Code A website that includes personal technical documentation and blogs.\n","categories":"","description":"","excerpt":" LuoHui's website! Learn More Source Code A website that includes …","ref":"/","tags":"","title":"Website"},{"body":"文章概述 本章介绍了如何使用 Hugo 管理个人站点内容，并部署在 GitHub Pages 上以供浏览。\nGitHub 上创建 website 仓库用于存储站点相关的内容。\n使用 Hugo 组织、管理并生成站点\n在 GitHub Pages 托管个人站点\n准备 website 仓库 登录 GitHub 创建 website 仓库 仓库名称：website 描述（可选）：对该存储库的简单描述 公开的 或 私人的：选择该存储库的可见性及谁可以提交更改等。 初始化存储库 README 文件（可选） 选择 .gitignore 和 license 使用 GitHub Desktop 客户端将存储库 clone 到本地 使用 Hugo 管理站点内容 步骤 1：安装 Hugo $ brew install hugo 验证安装\n$ hugo version Hugo Static Site Generator v0.74.1/extended darwin/amd64 BuildDate: unknown 步骤 2：初始化站点 $ cd website $ hugo new site --force . $ tree -L 1 . . ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── static └── themes 关于目录结构的更多信息，请参考目录结构说明\n步骤 3：为站点添加主题 themes.gohugo.io 提供了可选的主题列表，本示例中使用了 Ananke theme\n$ git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke $ echo 'theme = \"ananke\"' \u003e\u003e config.toml 步骤 4：为站点添加内容 使用 hugo new [path] 命令为站点添加内容，该命令以 content 作为根目录创建指定的文件，并将 archetypes/default.md 作为内容模板。\n$ hugo new blog/my-first-post.md $ cat content/blog/my-first-post.md --- title: \"My First Post\" date: 2020-07-15T21:47:19+08:00 draft: true --- 提示：关于如何管理内容，请参考内容管理；关于如何修改内容根目录，请参考 Configuration Settings - contentDir；阅读 archetypes 了解更多相关信息。\n可选：使用 Hugo server 预览站点 启动 Hugo server\n$ hugo server -D 访问 http://localhost:1313/ 预览站点\n构建并托管到 GitHub Pages 示例中使用 GitHub \u003cusername\u003e.github.io master 分支作为托管存储库\n步骤 1：修改站点配置文件 编辑 config.toml 文件，替换如下内容：\nbaseURL = \"https://\u003cusername\u003e.github.io\" title = \"Website\" theme = [\"ananke\"] 步骤 2：构建静态文件 $ hugo --gc 上述命令将在 public 目录下生成站点文件。\n步骤 3：发布到 GitHub $ cd public $ git init $ git add -A $ git commit -m \"@website\" # 替换 \u003cusername\u003e 为你实际的 GitHub 用户名 $ git push -f https://github.com/\u003cusername\u003e/\u003cusername\u003e.github.io.git master 可选：使用自定义域名 替换 config.toml 中的 baseURL 为实际的自定义域名 创建 static/CNAME 文件，其内容为实际的自定义域名 重新构建发布站点 参考链接 https://gohugo.io/getting-started/quick-start/ https://gohugo.io/hosting-and-deployment/ ","categories":"","description":"","excerpt":"文章概述 本章介绍了如何使用 Hugo 管理个人站点内容，并部署在 GitHub Pages 上以供浏览。\nGitHub …","ref":"/blog/2020/07/15/personal-site/","tags":"","title":"基于 Hugo + GitHub Pages 快速搭建个人网站"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2020/07/15/my-first-post/","tags":"","title":"My First Post"},{"body":" About About website This website is generated by hugo and docsy theme.\n","categories":"","description":"","excerpt":" About About website This website is generated by hugo and docsy …","ref":"/about/","tags":"","title":"About"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]